[38;5;2m[i 0516 12:27:37.113627 96 compiler.py:951] Jittor(1.3.3.14) src: /home/prs01/miniconda3/envs/jittor/lib/python3.7/site-packages/jittor[m
[38;5;2m[i 0516 12:27:37.129678 96 compiler.py:952] g++ at /usr/bin/g++(9.4.0)[m
[38;5;2m[i 0516 12:27:37.129795 96 compiler.py:953] cache_path: /home/prs01/.cache/jittor/jt1.3.3/g++9.4.0/py3.7.13/Linux-5.4.0-10xc4/IntelRXeonRGolx7a/default[m
[38;5;2m[i 0516 12:27:37.163341 96 install_cuda.py:53] cuda_driver_version: [11, 6][m
[38;5;2m[i 0516 12:27:37.191728 96 __init__.py:411] Found /home/prs01/.cache/jittor/jtcuda/cuda11.2_cudnn8_linux/bin/nvcc(11.2.152) at /home/prs01/.cache/jittor/jtcuda/cuda11.2_cudnn8_linux/bin/nvcc.[m
[38;5;2m[i 0516 12:27:37.218058 96 __init__.py:411] Found addr2line(2.34) at /usr/bin/addr2line.[m
[38;5;2m[i 0516 12:27:37.423521 96 compiler.py:1006] cuda key:cu11.2.152_sm_86[m
[38;5;2m[i 0516 12:27:37.818067 96 __init__.py:227] Total mem: 251.56GB, using 16 procs for compiling.[m
[38;5;2m[i 0516 12:27:38.100050 96 jit_compiler.cc:28] Load cc_path: /usr/bin/g++[m
[38;5;2m[i 0516 12:27:38.314011 96 init.cc:62] Found cuda archs: [86,][m
[38;5;2m[i 0516 12:27:38.529842 96 compile_extern.py:516] mpicc not found, distribution disabled.[m
[38;5;2m[i 0516 12:27:38.706298 96 compile_extern.py:30] found /home/prs01/.cache/jittor/jtcuda/cuda11.2_cudnn8_linux/include/cublas.h[m
[38;5;2m[i 0516 12:27:38.726148 96 compile_extern.py:30] found /home/prs01/.cache/jittor/jtcuda/cuda11.2_cudnn8_linux/lib64/libcublas.so[m
[38;5;2m[i 0516 12:27:38.726535 96 compile_extern.py:30] found /home/prs01/.cache/jittor/jtcuda/cuda11.2_cudnn8_linux/lib64/libcublasLt.so.11[m
[38;5;2m[i 0516 12:27:40.745701 96 compile_extern.py:30] found /home/prs01/.cache/jittor/jtcuda/cuda11.2_cudnn8_linux/include/cudnn.h[m
[38;5;2m[i 0516 12:27:40.789107 96 compile_extern.py:30] found /home/prs01/.cache/jittor/jtcuda/cuda11.2_cudnn8_linux/lib64/libcudnn.so.8[m
[38;5;2m[i 0516 12:27:40.789302 96 compile_extern.py:30] found /home/prs01/.cache/jittor/jtcuda/cuda11.2_cudnn8_linux/lib64/libcudnn_ops_infer.so.8[m
[38;5;2m[i 0516 12:27:40.838497 96 compile_extern.py:30] found /home/prs01/.cache/jittor/jtcuda/cuda11.2_cudnn8_linux/lib64/libcudnn_ops_train.so.8[m
[38;5;2m[i 0516 12:27:40.844096 96 compile_extern.py:30] found /home/prs01/.cache/jittor/jtcuda/cuda11.2_cudnn8_linux/lib64/libcudnn_cnn_infer.so.8[m
[38;5;2m[i 0516 12:27:41.069883 96 compile_extern.py:30] found /home/prs01/.cache/jittor/jtcuda/cuda11.2_cudnn8_linux/lib64/libcudnn_cnn_train.so.8[m
[38;5;2m[i 0516 12:27:42.805663 96 compile_extern.py:30] found /home/prs01/.cache/jittor/jtcuda/cuda11.2_cudnn8_linux/include/curand.h[m
[38;5;2m[i 0516 12:27:42.866096 96 compile_extern.py:30] found /home/prs01/.cache/jittor/jtcuda/cuda11.2_cudnn8_linux/lib64/libcurand.so[m
[38;5;2m[i 0516 12:27:42.981921 96 compile_extern.py:30] found /home/prs01/.cache/jittor/jtcuda/cuda11.2_cudnn8_linux/include/cufft.h[m
[38;5;2m[i 0516 12:27:43.041867 96 compile_extern.py:30] found /home/prs01/.cache/jittor/jtcuda/cuda11.2_cudnn8_linux/lib64/libcufft.so[m
[38;5;2m[i 0516 12:27:43.323119 96 cuda_flags.cc:32] CUDA enabled.[m
Loading data...
[38;5;2m[i 0516 12:27:45.528586 96 dataset.py:631] Found 5001 classes and 60012 images.[m
[38;5;2m[i 0516 12:27:47.531643 96 dataset.py:631] Found 5001 classes and 10002 images.[m
[38;5;2m[i 0516 12:27:48.849911 96 dataset.py:631] Found 5001 classes and 10002 images.[m
----------------- A new trial ---------------------
modelName ResNet learning rate: 0.0001 etaMin 1e-07 imgSize 224 batchSize 64 savedName 1.pkl criterion CrossEntropyLoss weight_decay 0.001 TMax 15
Training:   0%|          | 0/1600 [00:00<?, ?it/s][38;5;3m[w 0516 12:27:49.389300 96 grad.cc:77] grads[0] 'conv1.weight' doesn't have gradient. It will be set to zero: Var(32:1:2:2:i1:o1:s0:n1,float32,conv1.weight,0)[64,3,7,7,][m
[38;5;3m[w 0516 12:27:49.389343 96 grad.cc:77] grads[1] 'bn1.weight' doesn't have gradient. It will be set to zero: Var(40:1:3:3:i1:o2:s0:n1,float32,bn1.weight,0)[64,][m
[38;5;3m[w 0516 12:27:49.389353 96 grad.cc:77] grads[2] 'bn1.bias' doesn't have gradient. It will be set to zero: Var(45:1:2:2:i1:o1:s0:n1,float32,bn1.bias,0)[64,][m
[38;5;3m[w 0516 12:27:49.389367 96 grad.cc:77] grads[3] 'layer1.0.conv1.weight' doesn't have gradient. It will be set to zero: Var(140:1:3:3:i1:o2:s0:n1,float32,layer1.0.conv1.weight,0)[64,64,1,1,][m
[38;5;3m[w 0516 12:27:49.389377 96 grad.cc:77] grads[4] 'layer1.0.bn1.weight' doesn't have gradient. It will be set to zero: Var(148:1:3:3:i1:o2:s0:n1,float32,layer1.0.bn1.weight,0)[64,][m
[38;5;3m[w 0516 12:27:49.389385 96 grad.cc:77] grads[5] 'layer1.0.bn1.bias' doesn't have gradient. It will be set to zero: Var(153:1:2:2:i1:o1:s0:n1,float32,layer1.0.bn1.bias,0)[64,][m
[38;5;3m[w 0516 12:27:49.389393 96 grad.cc:77] grads[6] 'layer1.0.conv2.weight' doesn't have gradient. It will be set to zero: Var(194:1:3:3:i1:o2:s0:n1,float32,layer1.0.conv2.weight,0)[64,64,3,3,][m
[38;5;3m[w 0516 12:27:49.389402 96 grad.cc:77] grads[7] 'layer1.0.bn2.weight' doesn't have gradient. It will be set to zero: Var(202:1:3:3:i1:o2:s0:n1,float32,layer1.0.bn2.weight,0)[64,][m
[38;5;3m[w 0516 12:27:49.389410 96 grad.cc:77] grads[8] 'layer1.0.bn2.bias' doesn't have gradient. It will be set to zero: Var(207:1:2:2:i1:o1:s0:n1,float32,layer1.0.bn2.bias,0)[64,][m
[38;5;3m[w 0516 12:27:49.389418 96 grad.cc:77] grads[9] 'layer1.0.conv3.weight' doesn't have gradient. It will be set to zero: Var(248:1:3:3:i1:o2:s0:n1,float32,layer1.0.conv3.weight,0)[256,64,1,1,][m
[38;5;3m[w 0516 12:27:49.389427 96 grad.cc:77] grads[10] 'layer1.0.bn3.weight' doesn't have gradient. It will be set to zero: Var(256:1:3:3:i1:o2:s0:n1,float32,layer1.0.bn3.weight,0)[256,][m
[38;5;3m[w 0516 12:27:49.389434 96 grad.cc:77] grads[11] 'layer1.0.bn3.bias' doesn't have gradient. It will be set to zero: Var(261:1:2:2:i1:o1:s0:n1,float32,layer1.0.bn3.bias,0)[256,][m
[38;5;3m[w 0516 12:27:49.389442 96 grad.cc:77] grads[12] 'layer1.0.downsample.0.weight' doesn't have gradient. It will be set to zero: Var(86:1:3:3:i1:o2:s0:n1,float32,layer1.0.downsample.0.weight,0)[256,64,1,1,][m
[38;5;3m[w 0516 12:27:49.389453 96 grad.cc:77] grads[13] 'layer1.0.downsample.1.weight' doesn't have gradient. It will be set to zero: Var(94:1:3:3:i1:o2:s0:n1,float32,layer1.0.downsample.1.weight,0)[256,][m
[38;5;3m[w 0516 12:27:49.398849 96 grad.cc:77] grads[14] 'layer1.0.downsample.1.bias' doesn't have gradient. It will be set to zero: Var(99:1:2:2:i1:o1:s0:n1,float32,layer1.0.downsample.1.bias,0)[256,][m
[38;5;3m[w 0516 12:27:49.398859 96 grad.cc:77] grads[15] 'layer1.1.conv1.weight' doesn't have gradient. It will be set to zero: Var(302:1:3:3:i1:o2:s0:n1,float32,layer1.1.conv1.weight,0)[64,256,1,1,][m
[38;5;3m[w 0516 12:27:49.398868 96 grad.cc:77] grads[16] 'layer1.1.bn1.weight' doesn't have gradient. It will be set to zero: Var(310:1:3:3:i1:o2:s0:n1,float32,layer1.1.bn1.weight,0)[64,][m
[38;5;3m[w 0516 12:27:49.398876 96 grad.cc:77] grads[17] 'layer1.1.bn1.bias' doesn't have gradient. It will be set to zero: Var(315:1:2:2:i1:o1:s0:n1,float32,layer1.1.bn1.bias,0)[64,][m
[38;5;3m[w 0516 12:27:49.398884 96 grad.cc:77] grads[18] 'layer1.1.conv2.weight' doesn't have gradient. It will be set to zero: Var(356:1:3:3:i1:o2:s0:n1,float32,layer1.1.conv2.weight,0)[64,64,3,3,][m
[38;5;3m[w 0516 12:27:49.398892 96 grad.cc:77] grads[19] 'layer1.1.bn2.weight' doesn't have gradient. It will be set to zero: Var(364:1:3:3:i1:o2:s0:n1,float32,layer1.1.bn2.weight,0)[64,][m
[38;5;3m[w 0516 12:27:49.398900 96 grad.cc:77] grads[20] 'layer1.1.bn2.bias' doesn't have gradient. It will be set to zero: Var(369:1:2:2:i1:o1:s0:n1,float32,layer1.1.bn2.bias,0)[64,][m
[38;5;3m[w 0516 12:27:49.398907 96 grad.cc:77] grads[21] 'layer1.1.conv3.weight' doesn't have gradient. It will be set to zero: Var(410:1:3:3:i1:o2:s0:n1,float32,layer1.1.conv3.weight,0)[256,64,1,1,][m
[38;5;3m[w 0516 12:27:49.398918 96 grad.cc:77] grads[22] 'layer1.1.bn3.weight' doesn't have gradient. It will be set to zero: Var(418:1:3:3:i1:o2:s0:n1,float32,layer1.1.bn3.weight,0)[256,][m
[38;5;3m[w 0516 12:27:49.398928 96 grad.cc:77] grads[23] 'layer1.1.bn3.bias' doesn't have gradient. It will be set to zero: Var(423:1:2:2:i1:o1:s0:n1,float32,layer1.1.bn3.bias,0)[256,][m
[38;5;3m[w 0516 12:27:49.398941 96 grad.cc:77] grads[24] 'layer1.2.conv1.weight' doesn't have gradient. It will be set to zero: Var(464:1:3:3:i1:o2:s0:n1,float32,layer1.2.conv1.weight,0)[64,256,1,1,][m
[38;5;3m[w 0516 12:27:49.398948 96 grad.cc:77] grads[25] 'layer1.2.bn1.weight' doesn't have gradient. It will be set to zero: Var(472:1:3:3:i1:o2:s0:n1,float32,layer1.2.bn1.weight,0)[64,][m
[38;5;3m[w 0516 12:27:49.398956 96 grad.cc:77] grads[26] 'layer1.2.bn1.bias' doesn't have gradient. It will be set to zero: Var(477:1:2:2:i1:o1:s0:n1,float32,layer1.2.bn1.bias,0)[64,][m
[38;5;3m[w 0516 12:27:49.398963 96 grad.cc:77] grads[27] 'layer1.2.conv2.weight' doesn't have gradient. It will be set to zero: Var(518:1:3:3:i1:o2:s0:n1,float32,layer1.2.conv2.weight,0)[64,64,3,3,][m
[38;5;3m[w 0516 12:27:49.398972 96 grad.cc:77] grads[28] 'layer1.2.bn2.weight' doesn't have gradient. It will be set to zero: Var(526:1:3:3:i1:o2:s0:n1,float32,layer1.2.bn2.weight,0)[64,][m
[38;5;3m[w 0516 12:27:49.398980 96 grad.cc:77] grads[29] 'layer1.2.bn2.bias' doesn't have gradient. It will be set to zero: Var(531:1:2:2:i1:o1:s0:n1,float32,layer1.2.bn2.bias,0)[64,][m
[38;5;3m[w 0516 12:27:49.398988 96 grad.cc:77] grads[30] 'layer1.2.conv3.weight' doesn't have gradient. It will be set to zero: Var(572:1:3:3:i1:o2:s0:n1,float32,layer1.2.conv3.weight,0)[256,64,1,1,][m
[38;5;3m[w 0516 12:27:49.398996 96 grad.cc:77] grads[31] 'layer1.2.bn3.weight' doesn't have gradient. It will be set to zero: Var(580:1:3:3:i1:o2:s0:n1,float32,layer1.2.bn3.weight,0)[256,][m
[38;5;3m[w 0516 12:27:49.399004 96 grad.cc:77] grads[32] 'layer1.2.bn3.bias' doesn't have gradient. It will be set to zero: Var(585:1:2:2:i1:o1:s0:n1,float32,layer1.2.bn3.bias,0)[256,][m
[38;5;3m[w 0516 12:27:49.399012 96 grad.cc:77] grads[33] 'layer2.0.conv1.weight' doesn't have gradient. It will be set to zero: Var(680:1:3:3:i1:o2:s0:n1,float32,layer2.0.conv1.weight,0)[128,256,1,1,][m
[38;5;3m[w 0516 12:27:49.399021 96 grad.cc:77] grads[34] 'layer2.0.bn1.weight' doesn't have gradient. It will be set to zero: Var(688:1:3:3:i1:o2:s0:n1,float32,layer2.0.bn1.weight,0)[128,][m
[38;5;3m[w 0516 12:27:49.399029 96 grad.cc:77] grads[35] 'layer2.0.bn1.bias' doesn't have gradient. It will be set to zero: Var(693:1:2:2:i1:o1:s0:n1,float32,layer2.0.bn1.bias,0)[128,][m
[38;5;3m[w 0516 12:27:49.399037 96 grad.cc:77] grads[36] 'layer2.0.conv2.weight' doesn't have gradient. It will be set to zero: Var(734:1:3:3:i1:o2:s0:n1,float32,layer2.0.conv2.weight,0)[128,128,3,3,][m
[38;5;3m[w 0516 12:27:49.399046 96 grad.cc:77] grads[37] 'layer2.0.bn2.weight' doesn't have gradient. It will be set to zero: Var(742:1:3:3:i1:o2:s0:n1,float32,layer2.0.bn2.weight,0)[128,][m
[38;5;3m[w 0516 12:27:49.399054 96 grad.cc:77] grads[38] 'layer2.0.bn2.bias' doesn't have gradient. It will be set to zero: Var(747:1:2:2:i1:o1:s0:n1,float32,layer2.0.bn2.bias,0)[128,][m
[38;5;3m[w 0516 12:27:49.399062 96 grad.cc:77] grads[39] 'layer2.0.conv3.weight' doesn't have gradient. It will be set to zero: Var(788:1:3:3:i1:o2:s0:n1,float32,layer2.0.conv3.weight,0)[512,128,1,1,][m
[38;5;3m[w 0516 12:27:49.399070 96 grad.cc:77] grads[40] 'layer2.0.bn3.weight' doesn't have gradient. It will be set to zero: Var(796:1:3:3:i1:o2:s0:n1,float32,layer2.0.bn3.weight,0)[512,][m
[38;5;3m[w 0516 12:27:49.399078 96 grad.cc:77] grads[41] 'layer2.0.bn3.bias' doesn't have gradient. It will be set to zero: Var(801:1:2:2:i1:o1:s0:n1,float32,layer2.0.bn3.bias,0)[512,][m
[38;5;3m[w 0516 12:27:49.399086 96 grad.cc:77] grads[42] 'layer2.0.downsample.0.weight' doesn't have gradient. It will be set to zero: Var(626:1:3:3:i1:o2:s0:n1,float32,layer2.0.downsample.0.weight,0)[512,256,1,1,][m
[38;5;3m[w 0516 12:27:49.399094 96 grad.cc:77] grads[43] 'layer2.0.downsample.1.weight' doesn't have gradient. It will be set to zero: Var(634:1:3:3:i1:o2:s0:n1,float32,layer2.0.downsample.1.weight,0)[512,][m
[38;5;3m[w 0516 12:27:49.399105 96 grad.cc:77] grads[44] 'layer2.0.downsample.1.bias' doesn't have gradient. It will be set to zero: Var(639:1:2:2:i1:o1:s0:n1,float32,layer2.0.downsample.1.bias,0)[512,][m
[38;5;3m[w 0516 12:27:49.399114 96 grad.cc:77] grads[45] 'layer2.1.conv1.weight' doesn't have gradient. It will be set to zero: Var(842:1:3:3:i1:o2:s0:n1,float32,layer2.1.conv1.weight,0)[128,512,1,1,][m
[38;5;3m[w 0516 12:27:49.399123 96 grad.cc:77] grads[46] 'layer2.1.bn1.weight' doesn't have gradient. It will be set to zero: Var(850:1:3:3:i1:o2:s0:n1,float32,layer2.1.bn1.weight,0)[128,][m
[38;5;3m[w 0516 12:27:49.399130 96 grad.cc:77] grads[47] 'layer2.1.bn1.bias' doesn't have gradient. It will be set to zero: Var(855:1:2:2:i1:o1:s0:n1,float32,layer2.1.bn1.bias,0)[128,][m
[38;5;3m[w 0516 12:27:49.399138 96 grad.cc:77] grads[48] 'layer2.1.conv2.weight' doesn't have gradient. It will be set to zero: Var(896:1:3:3:i1:o2:s0:n1,float32,layer2.1.conv2.weight,0)[128,128,3,3,][m
[38;5;3m[w 0516 12:27:49.399147 96 grad.cc:77] grads[49] 'layer2.1.bn2.weight' doesn't have gradient. It will be set to zero: Var(904:1:3:3:i1:o2:s0:n1,float32,layer2.1.bn2.weight,0)[128,][m
[38;5;3m[w 0516 12:27:49.399154 96 grad.cc:77] grads[50] 'layer2.1.bn2.bias' doesn't have gradient. It will be set to zero: Var(909:1:2:2:i1:o1:s0:n1,float32,layer2.1.bn2.bias,0)[128,][m
[38;5;3m[w 0516 12:27:49.399162 96 grad.cc:77] grads[51] 'layer2.1.conv3.weight' doesn't have gradient. It will be set to zero: Var(950:1:3:3:i1:o2:s0:n1,float32,layer2.1.conv3.weight,0)[512,128,1,1,][m
[38;5;3m[w 0516 12:27:49.399172 96 grad.cc:77] grads[52] 'layer2.1.bn3.weight' doesn't have gradient. It will be set to zero: Var(958:1:3:3:i1:o2:s0:n1,float32,layer2.1.bn3.weight,0)[512,][m
[38;5;3m[w 0516 12:27:49.399179 96 grad.cc:77] grads[53] 'layer2.1.bn3.bias' doesn't have gradient. It will be set to zero: Var(963:1:2:2:i1:o1:s0:n1,float32,layer2.1.bn3.bias,0)[512,][m
[38;5;3m[w 0516 12:27:49.399188 96 grad.cc:77] grads[54] 'layer2.2.conv1.weight' doesn't have gradient. It will be set to zero: Var(1004:1:3:3:i1:o2:s0:n1,float32,layer2.2.conv1.weight,0)[128,512,1,1,][m
[38;5;3m[w 0516 12:27:49.399197 96 grad.cc:77] grads[55] 'layer2.2.bn1.weight' doesn't have gradient. It will be set to zero: Var(1012:1:3:3:i1:o2:s0:n1,float32,layer2.2.bn1.weight,0)[128,][m
[38;5;3m[w 0516 12:27:49.399205 96 grad.cc:77] grads[56] 'layer2.2.bn1.bias' doesn't have gradient. It will be set to zero: Var(1017:1:2:2:i1:o1:s0:n1,float32,layer2.2.bn1.bias,0)[128,][m
[38;5;3m[w 0516 12:27:49.399215 96 grad.cc:77] grads[57] 'layer2.2.conv2.weight' doesn't have gradient. It will be set to zero: Var(1058:1:3:3:i1:o2:s0:n1,float32,layer2.2.conv2.weight,0)[128,128,3,3,][m
[38;5;3m[w 0516 12:27:49.399713 96 grad.cc:77] grads[58] 'layer2.2.bn2.weight' doesn't have gradient. It will be set to zero: Var(1066:1:3:3:i1:o2:s0:n1,float32,layer2.2.bn2.weight,0)[128,][m
[38;5;3m[w 0516 12:27:49.399724 96 grad.cc:77] grads[59] 'layer2.2.bn2.bias' doesn't have gradient. It will be set to zero: Var(1071:1:2:2:i1:o1:s0:n1,float32,layer2.2.bn2.bias,0)[128,][m
[38;5;3m[w 0516 12:27:49.399733 96 grad.cc:77] grads[60] 'layer2.2.conv3.weight' doesn't have gradient. It will be set to zero: Var(1112:1:3:3:i1:o2:s0:n1,float32,layer2.2.conv3.weight,0)[512,128,1,1,][m
[38;5;3m[w 0516 12:27:49.399744 96 grad.cc:77] grads[61] 'layer2.2.bn3.weight' doesn't have gradient. It will be set to zero: Var(1120:1:3:3:i1:o2:s0:n1,float32,layer2.2.bn3.weight,0)[512,][m
[38;5;3m[w 0516 12:27:49.399756 96 grad.cc:77] grads[62] 'layer2.2.bn3.bias' doesn't have gradient. It will be set to zero: Var(1125:1:2:2:i1:o1:s0:n1,float32,layer2.2.bn3.bias,0)[512,][m
[38;5;3m[w 0516 12:27:49.399767 96 grad.cc:77] grads[63] 'layer2.3.conv1.weight' doesn't have gradient. It will be set to zero: Var(1166:1:3:3:i1:o2:s0:n1,float32,layer2.3.conv1.weight,0)[128,512,1,1,][m
[38;5;3m[w 0516 12:27:49.399775 96 grad.cc:77] grads[64] 'layer2.3.bn1.weight' doesn't have gradient. It will be set to zero: Var(1174:1:3:3:i1:o2:s0:n1,float32,layer2.3.bn1.weight,0)[128,][m
[38;5;3m[w 0516 12:27:49.399787 96 grad.cc:77] grads[65] 'layer2.3.bn1.bias' doesn't have gradient. It will be set to zero: Var(1179:1:2:2:i1:o1:s0:n1,float32,layer2.3.bn1.bias,0)[128,][m
[38;5;3m[w 0516 12:27:49.399795 96 grad.cc:77] grads[66] 'layer2.3.conv2.weight' doesn't have gradient. It will be set to zero: Var(1220:1:3:3:i1:o2:s0:n1,float32,layer2.3.conv2.weight,0)[128,128,3,3,][m
[38;5;3m[w 0516 12:27:49.399804 96 grad.cc:77] grads[67] 'layer2.3.bn2.weight' doesn't have gradient. It will be set to zero: Var(1228:1:3:3:i1:o2:s0:n1,float32,layer2.3.bn2.weight,0)[128,][m
[38;5;3m[w 0516 12:27:49.399813 96 grad.cc:77] grads[68] 'layer2.3.bn2.bias' doesn't have gradient. It will be set to zero: Var(1233:1:2:2:i1:o1:s0:n1,float32,layer2.3.bn2.bias,0)[128,][m
[38;5;3m[w 0516 12:27:49.399821 96 grad.cc:77] grads[69] 'layer2.3.conv3.weight' doesn't have gradient. It will be set to zero: Var(1274:1:3:3:i1:o2:s0:n1,float32,layer2.3.conv3.weight,0)[512,128,1,1,][m
[38;5;3m[w 0516 12:27:49.399830 96 grad.cc:77] grads[70] 'layer2.3.bn3.weight' doesn't have gradient. It will be set to zero: Var(1282:1:3:3:i1:o2:s0:n1,float32,layer2.3.bn3.weight,0)[512,][m
[38;5;3m[w 0516 12:27:49.399839 96 grad.cc:77] grads[71] 'layer2.3.bn3.bias' doesn't have gradient. It will be set to zero: Var(1287:1:2:2:i1:o1:s0:n1,float32,layer2.3.bn3.bias,0)[512,][m
[38;5;3m[w 0516 12:27:49.399846 96 grad.cc:77] grads[72] 'layer3.0.conv1.weight' doesn't have gradient. It will be set to zero: Var(1382:1:3:3:i1:o2:s0:n1,float32,layer3.0.conv1.weight,0)[256,512,1,1,][m
[38;5;3m[w 0516 12:27:49.399856 96 grad.cc:77] grads[73] 'layer3.0.bn1.weight' doesn't have gradient. It will be set to zero: Var(1390:1:3:3:i1:o2:s0:n1,float32,layer3.0.bn1.weight,0)[256,][m
[38;5;3m[w 0516 12:27:49.399864 96 grad.cc:77] grads[74] 'layer3.0.bn1.bias' doesn't have gradient. It will be set to zero: Var(1395:1:2:2:i1:o1:s0:n1,float32,layer3.0.bn1.bias,0)[256,][m
[38;5;3m[w 0516 12:27:49.399873 96 grad.cc:77] grads[75] 'layer3.0.conv2.weight' doesn't have gradient. It will be set to zero: Var(1436:1:3:3:i1:o2:s0:n1,float32,layer3.0.conv2.weight,0)[256,256,3,3,][m
[38;5;3m[w 0516 12:27:49.399882 96 grad.cc:77] grads[76] 'layer3.0.bn2.weight' doesn't have gradient. It will be set to zero: Var(1444:1:3:3:i1:o2:s0:n1,float32,layer3.0.bn2.weight,0)[256,][m
[38;5;3m[w 0516 12:27:49.399890 96 grad.cc:77] grads[77] 'layer3.0.bn2.bias' doesn't have gradient. It will be set to zero: Var(1449:1:2:2:i1:o1:s0:n1,float32,layer3.0.bn2.bias,0)[256,][m
[38;5;3m[w 0516 12:27:49.399898 96 grad.cc:77] grads[78] 'layer3.0.conv3.weight' doesn't have gradient. It will be set to zero: Var(1490:1:3:3:i1:o2:s0:n1,float32,layer3.0.conv3.weight,0)[1024,256,1,1,][m
[38;5;3m[w 0516 12:27:49.399907 96 grad.cc:77] grads[79] 'layer3.0.bn3.weight' doesn't have gradient. It will be set to zero: Var(1498:1:3:3:i1:o2:s0:n1,float32,layer3.0.bn3.weight,0)[1024,][m
[38;5;3m[w 0516 12:27:49.399915 96 grad.cc:77] grads[80] 'layer3.0.bn3.bias' doesn't have gradient. It will be set to zero: Var(1503:1:2:2:i1:o1:s0:n1,float32,layer3.0.bn3.bias,0)[1024,][m
[38;5;3m[w 0516 12:27:49.399923 96 grad.cc:77] grads[81] 'layer3.0.downsample.0.weight' doesn't have gradient. It will be set to zero: Var(1328:1:3:3:i1:o2:s0:n1,float32,layer3.0.downsample.0.weight,0)[1024,512,1,1,][m
[38;5;3m[w 0516 12:27:49.399932 96 grad.cc:77] grads[82] 'layer3.0.downsample.1.weight' doesn't have gradient. It will be set to zero: Var(1336:1:3:3:i1:o2:s0:n1,float32,layer3.0.downsample.1.weight,0)[1024,][m
[38;5;3m[w 0516 12:27:49.399940 96 grad.cc:77] grads[83] 'layer3.0.downsample.1.bias' doesn't have gradient. It will be set to zero: Var(1341:1:2:2:i1:o1:s0:n1,float32,layer3.0.downsample.1.bias,0)[1024,][m
[38;5;3m[w 0516 12:27:49.399949 96 grad.cc:77] grads[84] 'layer3.1.conv1.weight' doesn't have gradient. It will be set to zero: Var(1544:1:3:3:i1:o2:s0:n1,float32,layer3.1.conv1.weight,0)[256,1024,1,1,][m
[38;5;3m[w 0516 12:27:49.399957 96 grad.cc:77] grads[85] 'layer3.1.bn1.weight' doesn't have gradient. It will be set to zero: Var(1552:1:3:3:i1:o2:s0:n1,float32,layer3.1.bn1.weight,0)[256,][m
[38;5;3m[w 0516 12:27:49.399968 96 grad.cc:77] grads[86] 'layer3.1.bn1.bias' doesn't have gradient. It will be set to zero: Var(1557:1:2:2:i1:o1:s0:n1,float32,layer3.1.bn1.bias,0)[256,][m
[38;5;3m[w 0516 12:27:49.399977 96 grad.cc:77] grads[87] 'layer3.1.conv2.weight' doesn't have gradient. It will be set to zero: Var(1598:1:3:3:i1:o2:s0:n1,float32,layer3.1.conv2.weight,0)[256,256,3,3,][m
[38;5;3m[w 0516 12:27:49.399985 96 grad.cc:77] grads[88] 'layer3.1.bn2.weight' doesn't have gradient. It will be set to zero: Var(1606:1:3:3:i1:o2:s0:n1,float32,layer3.1.bn2.weight,0)[256,][m
[38;5;3m[w 0516 12:27:49.399993 96 grad.cc:77] grads[89] 'layer3.1.bn2.bias' doesn't have gradient. It will be set to zero: Var(1611:1:2:2:i1:o1:s0:n1,float32,layer3.1.bn2.bias,0)[256,][m
[38;5;3m[w 0516 12:27:49.400001 96 grad.cc:77] grads[90] 'layer3.1.conv3.weight' doesn't have gradient. It will be set to zero: Var(1652:1:3:3:i1:o2:s0:n1,float32,layer3.1.conv3.weight,0)[1024,256,1,1,][m
[38;5;3m[w 0516 12:27:49.400009 96 grad.cc:77] grads[91] 'layer3.1.bn3.weight' doesn't have gradient. It will be set to zero: Var(1660:1:3:3:i1:o2:s0:n1,float32,layer3.1.bn3.weight,0)[1024,][m
[38;5;3m[w 0516 12:27:49.400017 96 grad.cc:77] grads[92] 'layer3.1.bn3.bias' doesn't have gradient. It will be set to zero: Var(1665:1:2:2:i1:o1:s0:n1,float32,layer3.1.bn3.bias,0)[1024,][m
[38;5;3m[w 0516 12:27:49.400027 96 grad.cc:77] grads[93] 'layer3.2.conv1.weight' doesn't have gradient. It will be set to zero: Var(1706:1:3:3:i1:o2:s0:n1,float32,layer3.2.conv1.weight,0)[256,1024,1,1,][m
[38;5;3m[w 0516 12:27:49.400035 96 grad.cc:77] grads[94] 'layer3.2.bn1.weight' doesn't have gradient. It will be set to zero: Var(1714:1:3:3:i1:o2:s0:n1,float32,layer3.2.bn1.weight,0)[256,][m
[38;5;3m[w 0516 12:27:49.400043 96 grad.cc:77] grads[95] 'layer3.2.bn1.bias' doesn't have gradient. It will be set to zero: Var(1719:1:2:2:i1:o1:s0:n1,float32,layer3.2.bn1.bias,0)[256,][m
[38;5;3m[w 0516 12:27:49.400052 96 grad.cc:77] grads[96] 'layer3.2.conv2.weight' doesn't have gradient. It will be set to zero: Var(1760:1:3:3:i1:o2:s0:n1,float32,layer3.2.conv2.weight,0)[256,256,3,3,][m
[38;5;3m[w 0516 12:27:49.400528 96 grad.cc:77] grads[97] 'layer3.2.bn2.weight' doesn't have gradient. It will be set to zero: Var(1768:1:3:3:i1:o2:s0:n1,float32,layer3.2.bn2.weight,0)[256,][m
[38;5;3m[w 0516 12:27:49.400537 96 grad.cc:77] grads[98] 'layer3.2.bn2.bias' doesn't have gradient. It will be set to zero: Var(1773:1:2:2:i1:o1:s0:n1,float32,layer3.2.bn2.bias,0)[256,][m
[38;5;3m[w 0516 12:27:49.400546 96 grad.cc:77] grads[99] 'layer3.2.conv3.weight' doesn't have gradient. It will be set to zero: Var(1814:1:3:3:i1:o2:s0:n1,float32,layer3.2.conv3.weight,0)[1024,256,1,1,][m
[38;5;3m[w 0516 12:27:49.400555 96 grad.cc:77] grads[100] 'layer3.2.bn3.weight' doesn't have gradient. It will be set to zero: Var(1822:1:3:3:i1:o2:s0:n1,float32,layer3.2.bn3.weight,0)[1024,][m
[38;5;3m[w 0516 12:27:49.400563 96 grad.cc:77] grads[101] 'layer3.2.bn3.bias' doesn't have gradient. It will be set to zero: Var(1827:1:2:2:i1:o1:s0:n1,float32,layer3.2.bn3.bias,0)[1024,][m
[38;5;3m[w 0516 12:27:49.400571 96 grad.cc:77] grads[102] 'layer3.3.conv1.weight' doesn't have gradient. It will be set to zero: Var(1868:1:3:3:i1:o2:s0:n1,float32,layer3.3.conv1.weight,0)[256,1024,1,1,][m
[38;5;3m[w 0516 12:27:49.400580 96 grad.cc:77] grads[103] 'layer3.3.bn1.weight' doesn't have gradient. It will be set to zero: Var(1876:1:3:3:i1:o2:s0:n1,float32,layer3.3.bn1.weight,0)[256,][m
[38;5;3m[w 0516 12:27:49.400587 96 grad.cc:77] grads[104] 'layer3.3.bn1.bias' doesn't have gradient. It will be set to zero: Var(1881:1:2:2:i1:o1:s0:n1,float32,layer3.3.bn1.bias,0)[256,][m
[38;5;3m[w 0516 12:27:49.400596 96 grad.cc:77] grads[105] 'layer3.3.conv2.weight' doesn't have gradient. It will be set to zero: Var(1922:1:3:3:i1:o2:s0:n1,float32,layer3.3.conv2.weight,0)[256,256,3,3,][m
[38;5;3m[w 0516 12:27:49.400610 96 grad.cc:77] grads[106] 'layer3.3.bn2.weight' doesn't have gradient. It will be set to zero: Var(1930:1:3:3:i1:o2:s0:n1,float32,layer3.3.bn2.weight,0)[256,][m
[38;5;3m[w 0516 12:27:49.400618 96 grad.cc:77] grads[107] 'layer3.3.bn2.bias' doesn't have gradient. It will be set to zero: Var(1935:1:2:2:i1:o1:s0:n1,float32,layer3.3.bn2.bias,0)[256,][m
[38;5;3m[w 0516 12:27:49.400627 96 grad.cc:77] grads[108] 'layer3.3.conv3.weight' doesn't have gradient. It will be set to zero: Var(1976:1:3:3:i1:o2:s0:n1,float32,layer3.3.conv3.weight,0)[1024,256,1,1,][m
[38;5;3m[w 0516 12:27:49.400637 96 grad.cc:77] grads[109] 'layer3.3.bn3.weight' doesn't have gradient. It will be set to zero: Var(1984:1:3:3:i1:o2:s0:n1,float32,layer3.3.bn3.weight,0)[1024,][m
[38;5;3m[w 0516 12:27:49.400645 96 grad.cc:77] grads[110] 'layer3.3.bn3.bias' doesn't have gradient. It will be set to zero: Var(1989:1:2:2:i1:o1:s0:n1,float32,layer3.3.bn3.bias,0)[1024,][m
[38;5;3m[w 0516 12:27:49.400653 96 grad.cc:77] grads[111] 'layer3.4.conv1.weight' doesn't have gradient. It will be set to zero: Var(2030:1:3:3:i1:o2:s0:n1,float32,layer3.4.conv1.weight,0)[256,1024,1,1,][m
[38;5;3m[w 0516 12:27:49.400661 96 grad.cc:77] grads[112] 'layer3.4.bn1.weight' doesn't have gradient. It will be set to zero: Var(2038:1:3:3:i1:o2:s0:n1,float32,layer3.4.bn1.weight,0)[256,][m
[38;5;3m[w 0516 12:27:49.400669 96 grad.cc:77] grads[113] 'layer3.4.bn1.bias' doesn't have gradient. It will be set to zero: Var(2043:1:2:2:i1:o1:s0:n1,float32,layer3.4.bn1.bias,0)[256,][m
[38;5;3m[w 0516 12:27:49.400677 96 grad.cc:77] grads[114] 'layer3.4.conv2.weight' doesn't have gradient. It will be set to zero: Var(2084:1:3:3:i1:o2:s0:n1,float32,layer3.4.conv2.weight,0)[256,256,3,3,][m
[38;5;3m[w 0516 12:27:49.400686 96 grad.cc:77] grads[115] 'layer3.4.bn2.weight' doesn't have gradient. It will be set to zero: Var(2092:1:3:3:i1:o2:s0:n1,float32,layer3.4.bn2.weight,0)[256,][m
[38;5;3m[w 0516 12:27:49.400695 96 grad.cc:77] grads[116] 'layer3.4.bn2.bias' doesn't have gradient. It will be set to zero: Var(2097:1:2:2:i1:o1:s0:n1,float32,layer3.4.bn2.bias,0)[256,][m
[38;5;3m[w 0516 12:27:49.400703 96 grad.cc:77] grads[117] 'layer3.4.conv3.weight' doesn't have gradient. It will be set to zero: Var(2138:1:3:3:i1:o2:s0:n1,float32,layer3.4.conv3.weight,0)[1024,256,1,1,][m
[38;5;3m[w 0516 12:27:49.400713 96 grad.cc:77] grads[118] 'layer3.4.bn3.weight' doesn't have gradient. It will be set to zero: Var(2146:1:3:3:i1:o2:s0:n1,float32,layer3.4.bn3.weight,0)[1024,][m
[38;5;3m[w 0516 12:27:49.400721 96 grad.cc:77] grads[119] 'layer3.4.bn3.bias' doesn't have gradient. It will be set to zero: Var(2151:1:2:2:i1:o1:s0:n1,float32,layer3.4.bn3.bias,0)[1024,][m
[38;5;3m[w 0516 12:27:49.400729 96 grad.cc:77] grads[120] 'layer3.5.conv1.weight' doesn't have gradient. It will be set to zero: Var(2192:1:3:3:i1:o2:s0:n1,float32,layer3.5.conv1.weight,0)[256,1024,1,1,][m
[38;5;3m[w 0516 12:27:49.400738 96 grad.cc:77] grads[121] 'layer3.5.bn1.weight' doesn't have gradient. It will be set to zero: Var(2200:1:3:3:i1:o2:s0:n1,float32,layer3.5.bn1.weight,0)[256,][m
[38;5;3m[w 0516 12:27:49.400746 96 grad.cc:77] grads[122] 'layer3.5.bn1.bias' doesn't have gradient. It will be set to zero: Var(2205:1:2:2:i1:o1:s0:n1,float32,layer3.5.bn1.bias,0)[256,][m
[38;5;3m[w 0516 12:27:49.400754 96 grad.cc:77] grads[123] 'layer3.5.conv2.weight' doesn't have gradient. It will be set to zero: Var(2246:1:3:3:i1:o2:s0:n1,float32,layer3.5.conv2.weight,0)[256,256,3,3,][m
[38;5;3m[w 0516 12:27:49.400762 96 grad.cc:77] grads[124] 'layer3.5.bn2.weight' doesn't have gradient. It will be set to zero: Var(2254:1:3:3:i1:o2:s0:n1,float32,layer3.5.bn2.weight,0)[256,][m
[38;5;3m[w 0516 12:27:49.400771 96 grad.cc:77] grads[125] 'layer3.5.bn2.bias' doesn't have gradient. It will be set to zero: Var(2259:1:2:2:i1:o1:s0:n1,float32,layer3.5.bn2.bias,0)[256,][m
[38;5;3m[w 0516 12:27:49.400780 96 grad.cc:77] grads[126] 'layer3.5.conv3.weight' doesn't have gradient. It will be set to zero: Var(2300:1:3:3:i1:o2:s0:n1,float32,layer3.5.conv3.weight,0)[1024,256,1,1,][m
[38;5;3m[w 0516 12:27:49.400793 96 grad.cc:77] grads[127] 'layer3.5.bn3.weight' doesn't have gradient. It will be set to zero: Var(2308:1:3:3:i1:o2:s0:n1,float32,layer3.5.bn3.weight,0)[1024,][m
[38;5;3m[w 0516 12:27:49.419844 96 grad.cc:77] grads[128] 'layer3.5.bn3.bias' doesn't have gradient. It will be set to zero: Var(2313:1:2:2:i1:o1:s0:n1,float32,layer3.5.bn3.bias,0)[1024,][m
[38;5;3m[w 0516 12:27:49.419872 96 grad.cc:77] grads[129] 'layer4.0.conv1.weight' doesn't have gradient. It will be set to zero: Var(2408:1:3:3:i1:o2:s0:n1,float32,layer4.0.conv1.weight,0)[512,1024,1,1,][m
[38;5;3m[w 0516 12:27:49.419890 96 grad.cc:77] grads[130] 'layer4.0.bn1.weight' doesn't have gradient. It will be set to zero: Var(2416:1:3:3:i1:o2:s0:n1,float32,layer4.0.bn1.weight,0)[512,][m
[38;5;3m[w 0516 12:27:49.419903 96 grad.cc:77] grads[131] 'layer4.0.bn1.bias' doesn't have gradient. It will be set to zero: Var(2421:1:2:2:i1:o1:s0:n1,float32,layer4.0.bn1.bias,0)[512,][m
[38;5;3m[w 0516 12:27:49.419917 96 grad.cc:77] grads[132] 'layer4.0.conv2.weight' doesn't have gradient. It will be set to zero: Var(2462:1:3:3:i1:o2:s0:n1,float32,layer4.0.conv2.weight,0)[512,512,3,3,][m
[38;5;3m[w 0516 12:27:49.430744 96 grad.cc:77] grads[133] 'layer4.0.bn2.weight' doesn't have gradient. It will be set to zero: Var(2470:1:3:3:i1:o2:s0:n1,float32,layer4.0.bn2.weight,0)[512,][m
[38;5;3m[w 0516 12:27:49.430781 96 grad.cc:77] grads[134] 'layer4.0.bn2.bias' doesn't have gradient. It will be set to zero: Var(2475:1:2:2:i1:o1:s0:n1,float32,layer4.0.bn2.bias,0)[512,][m
[38;5;3m[w 0516 12:27:49.430803 96 grad.cc:77] grads[135] 'layer4.0.conv3.weight' doesn't have gradient. It will be set to zero: Var(2516:1:3:3:i1:o2:s0:n1,float32,layer4.0.conv3.weight,0)[2048,512,1,1,][m
[38;5;3m[w 0516 12:27:49.430829 96 grad.cc:77] grads[136] 'layer4.0.bn3.weight' doesn't have gradient. It will be set to zero: Var(2524:1:3:3:i1:o2:s0:n1,float32,layer4.0.bn3.weight,0)[2048,][m
[38;5;3m[w 0516 12:27:49.430847 96 grad.cc:77] grads[137] 'layer4.0.bn3.bias' doesn't have gradient. It will be set to zero: Var(2529:1:2:2:i1:o1:s0:n1,float32,layer4.0.bn3.bias,0)[2048,][m
[38;5;3m[w 0516 12:27:49.430864 96 grad.cc:77] grads[138] 'layer4.0.downsample.0.weight' doesn't have gradient. It will be set to zero: Var(2354:1:3:3:i1:o2:s0:n1,float32,layer4.0.downsample.0.weight,0)[2048,1024,1,1,][m
[38;5;3m[w 0516 12:27:49.430897 96 grad.cc:77] grads[139] 'layer4.0.downsample.1.weight' doesn't have gradient. It will be set to zero: Var(2362:1:3:3:i1:o2:s0:n1,float32,layer4.0.downsample.1.weight,0)[2048,][m
[38;5;3m[w 0516 12:27:49.430909 96 grad.cc:77] grads[140] 'layer4.0.downsample.1.bias' doesn't have gradient. It will be set to zero: Var(2367:1:2:2:i1:o1:s0:n1,float32,layer4.0.downsample.1.bias,0)[2048,][m
[38;5;3m[w 0516 12:27:49.430921 96 grad.cc:77] grads[141] 'layer4.1.conv1.weight' doesn't have gradient. It will be set to zero: Var(2570:1:3:3:i1:o2:s0:n1,float32,layer4.1.conv1.weight,0)[512,2048,1,1,][m
[38;5;3m[w 0516 12:27:49.430934 96 grad.cc:77] grads[142] 'layer4.1.bn1.weight' doesn't have gradient. It will be set to zero: Var(2578:1:3:3:i1:o2:s0:n1,float32,layer4.1.bn1.weight,0)[512,][m
[38;5;3m[w 0516 12:27:49.430946 96 grad.cc:77] grads[143] 'layer4.1.bn1.bias' doesn't have gradient. It will be set to zero: Var(2583:1:2:2:i1:o1:s0:n1,float32,layer4.1.bn1.bias,0)[512,][m
[38;5;3m[w 0516 12:27:49.430957 96 grad.cc:77] grads[144] 'layer4.1.conv2.weight' doesn't have gradient. It will be set to zero: Var(2624:1:3:3:i1:o2:s0:n1,float32,layer4.1.conv2.weight,0)[512,512,3,3,][m
[38;5;3m[w 0516 12:27:49.430970 96 grad.cc:77] grads[145] 'layer4.1.bn2.weight' doesn't have gradient. It will be set to zero: Var(2632:1:3:3:i1:o2:s0:n1,float32,layer4.1.bn2.weight,0)[512,][m
[38;5;3m[w 0516 12:27:49.430981 96 grad.cc:77] grads[146] 'layer4.1.bn2.bias' doesn't have gradient. It will be set to zero: Var(2637:1:2:2:i1:o1:s0:n1,float32,layer4.1.bn2.bias,0)[512,][m
[38;5;3m[w 0516 12:27:49.430993 96 grad.cc:77] grads[147] 'layer4.1.conv3.weight' doesn't have gradient. It will be set to zero: Var(2678:1:3:3:i1:o2:s0:n1,float32,layer4.1.conv3.weight,0)[2048,512,1,1,][m
[38;5;3m[w 0516 12:27:49.431012 96 grad.cc:77] grads[148] 'layer4.1.bn3.weight' doesn't have gradient. It will be set to zero: Var(2686:1:3:3:i1:o2:s0:n1,float32,layer4.1.bn3.weight,0)[2048,][m
[38;5;3m[w 0516 12:27:49.431025 96 grad.cc:77] grads[149] 'layer4.1.bn3.bias' doesn't have gradient. It will be set to zero: Var(2691:1:2:2:i1:o1:s0:n1,float32,layer4.1.bn3.bias,0)[2048,][m
[38;5;3m[w 0516 12:27:49.431037 96 grad.cc:77] grads[150] 'layer4.2.conv1.weight' doesn't have gradient. It will be set to zero: Var(2732:1:3:3:i1:o2:s0:n1,float32,layer4.2.conv1.weight,0)[512,2048,1,1,][m
[38;5;3m[w 0516 12:27:49.431049 96 grad.cc:77] grads[151] 'layer4.2.bn1.weight' doesn't have gradient. It will be set to zero: Var(2740:1:3:3:i1:o2:s0:n1,float32,layer4.2.bn1.weight,0)[512,][m
[38;5;3m[w 0516 12:27:49.431062 96 grad.cc:77] grads[152] 'layer4.2.bn1.bias' doesn't have gradient. It will be set to zero: Var(2745:1:2:2:i1:o1:s0:n1,float32,layer4.2.bn1.bias,0)[512,][m
[38;5;3m[w 0516 12:27:49.431074 96 grad.cc:77] grads[153] 'layer4.2.conv2.weight' doesn't have gradient. It will be set to zero: Var(2786:1:3:3:i1:o2:s0:n1,float32,layer4.2.conv2.weight,0)[512,512,3,3,][m
[38;5;3m[w 0516 12:27:49.431086 96 grad.cc:77] grads[154] 'layer4.2.bn2.weight' doesn't have gradient. It will be set to zero: Var(2794:1:3:3:i1:o2:s0:n1,float32,layer4.2.bn2.weight,0)[512,][m
[38;5;3m[w 0516 12:27:49.431098 96 grad.cc:77] grads[155] 'layer4.2.bn2.bias' doesn't have gradient. It will be set to zero: Var(2799:1:2:2:i1:o1:s0:n1,float32,layer4.2.bn2.bias,0)[512,][m
[38;5;3m[w 0516 12:27:49.440676 96 grad.cc:77] grads[156] 'layer4.2.conv3.weight' doesn't have gradient. It will be set to zero: Var(2840:1:3:3:i1:o2:s0:n1,float32,layer4.2.conv3.weight,0)[2048,512,1,1,][m
[38;5;3m[w 0516 12:27:49.440708 96 grad.cc:77] grads[157] 'layer4.2.bn3.weight' doesn't have gradient. It will be set to zero: Var(2848:1:3:3:i1:o2:s0:n1,float32,layer4.2.bn3.weight,0)[2048,][m
[38;5;3m[w 0516 12:27:49.440722 96 grad.cc:77] grads[158] 'layer4.2.bn3.bias' doesn't have gradient. It will be set to zero: Var(2853:1:2:2:i1:o1:s0:n1,float32,layer4.2.bn3.bias,0)[2048,][m
[38;5;3m[w 0516 12:27:49.440735 96 grad.cc:77] grads[159] 'fc.weight' doesn't have gradient. It will be set to zero: Var(2877:1:2:2:i1:o1:s0:n1,float32,fc.weight,0)[5001,2048,][m
[38;5;3m[w 0516 12:27:49.440748 96 grad.cc:77] grads[160] 'fc.bias' doesn't have gradient. It will be set to zero: Var(2894:1:2:2:i1:o1:s0:n0,float32,fc.bias,0)[5001,][m
index: 0 loss: 8.656778
index: 200 loss: 8.631094
index: 400 loss: 8.541828
index: 600 loss: 8.399829
index: 800 loss: 8.193699
epoch: 0 validateAccuracy: 0.0016 trainAccuracy: 0.0003 delta: 0.0016
Training:   0%|          | 1/1600 [10:24<277:23:17, 624.51s/it]index: 0 loss: 7.854363
index: 200 loss: 7.681333
index: 400 loss: 7.284645
index: 600 loss: 7.0079546
index: 800 loss: 6.928318
epoch: 1 validateAccuracy: 0.0089 trainAccuracy: 0.0031 delta: 0.0073
Training:   0%|          | 2/1600 [18:23<239:05:46, 538.64s/it]index: 0 loss: 6.634422
index: 200 loss: 6.1976905
index: 400 loss: 6.059104
index: 600 loss: 5.7566185
index: 800 loss: 5.4428716
epoch: 2 validateAccuracy: 0.0336 trainAccuracy: 0.0173 delta: 0.0247
Training:   0%|          | 3/1600 [26:22<226:59:17, 511.68s/it]index: 0 loss: 5.4232564
index: 200 loss: 5.289452
index: 400 loss: 4.931075
index: 600 loss: 4.626519
index: 800 loss: 4.37699
epoch: 3 validateAccuracy: 0.0871 trainAccuracy: 0.0607 delta: 0.0535
Training:   0%|          | 4/1600 [34:20<220:50:17, 498.13s/it]index: 0 loss: 4.213131
index: 200 loss: 4.135347
index: 400 loss: 4.3732095
index: 600 loss: 4.1036544
index: 800 loss: 3.659598
epoch: 4 validateAccuracy: 0.1303 trainAccuracy: 0.1382 delta: 0.0432
Training:   0%|          | 5/1600 [42:17<217:28:07, 490.84s/it]index: 0 loss: 3.3047588
index: 200 loss: 3.4879797
index: 400 loss: 3.6958613
index: 600 loss: 3.2290614
index: 800 loss: 3.060991
epoch: 5 validateAccuracy: 0.1769 trainAccuracy: 0.2161 delta: 0.0466
Training:   0%|          | 6/1600 [50:19<216:00:31, 487.85s/it]index: 0 loss: 3.024934
index: 200 loss: 2.5958598
index: 400 loss: 2.7830467
index: 600 loss: 2.759295
index: 800 loss: 2.6631563
epoch: 6 validateAccuracy: 0.2139 trainAccuracy: 0.2902 delta: 0.037
Training:   0%|          | 7/1600 [58:17<214:21:46, 484.44s/it]index: 0 loss: 2.793655
index: 200 loss: 2.9886029
index: 400 loss: 2.9388373
index: 600 loss: 2.3234632
index: 800 loss: 2.4206746
epoch: 7 validateAccuracy: 0.2404 trainAccuracy: 0.3466 delta: 0.0265
Training:   0%|          | 8/1600 [1:06:15<213:19:03, 482.38s/it]index: 0 loss: 2.5184944
index: 200 loss: 2.1451778
index: 400 loss: 2.3369904
index: 600 loss: 2.337898
index: 800 loss: 2.4486942
epoch: 8 validateAccuracy: 0.2594 trainAccuracy: 0.4011 delta: 0.019
Training:   1%|          | 9/1600 [1:14:17<213:12:33, 482.43s/it]index: 0 loss: 2.5096338
index: 200 loss: 2.08146
index: 400 loss: 2.2304506
index: 600 loss: 1.9245874
index: 800 loss: 2.12711
epoch: 9 validateAccuracy: 0.272 trainAccuracy: 0.4547 delta: 0.0126
Training:   1%|          | 10/1600 [1:22:17<212:42:03, 481.59s/it]index: 0 loss: 1.8818846
index: 200 loss: 1.884239
index: 400 loss: 2.2680957
index: 600 loss: 1.9346311
index: 800 loss: 2.0927138
epoch: 10 validateAccuracy: 0.3079 trainAccuracy: 0.5023 delta: 0.0359
Training:   1%|          | 11/1600 [1:30:17<212:21:33, 481.12s/it]index: 0 loss: 1.735713
index: 200 loss: 1.7684503
index: 400 loss: 1.8335335
index: 600 loss: 2.248131
index: 800 loss: 1.7277427
epoch: 11 validateAccuracy: 0.3094 trainAccuracy: 0.5483 delta: 0.0015
Training:   1%|          | 12/1600 [1:38:16<211:57:45, 480.52s/it]index: 0 loss: 1.8005836
index: 200 loss: 1.6137664
index: 400 loss: 1.7717435
index: 600 loss: 1.5548704
index: 800 loss: 1.5924816
epoch: 12 validateAccuracy: 0.3228 trainAccuracy: 0.5922 delta: 0.0134
Training:   1%|          | 13/1600 [1:46:17<211:48:43, 480.48s/it]index: 0 loss: 1.6843868
index: 200 loss: 1.6290827
index: 400 loss: 1.6251183
index: 600 loss: 1.7130743
index: 800 loss: 1.579717
epoch: 13 validateAccuracy: 0.3293 trainAccuracy: 0.6286 delta: 0.0065
Training:   1%|          | 14/1600 [1:54:16<211:32:22, 480.17s/it]index: 0 loss: 1.6056879
index: 200 loss: 1.580152
index: 400 loss: 1.5392284
index: 600 loss: 1.4369677
index: 800 loss: 1.3923653
epoch: 14 validateAccuracy: 0.3332 trainAccuracy: 0.6615 delta: 0.0039
Training:   1%|          | 15/1600 [2:02:16<211:18:18, 479.94s/it]index: 0 loss: 1.6194522
index: 200 loss: 1.5016373
index: 400 loss: 1.5271084
index: 600 loss: 1.2533704
index: 800 loss: 1.5675579
epoch: 15 validateAccuracy: 0.3398 trainAccuracy: 0.6799 delta: 0.0066
Training:   1%|          | 16/1600 [2:10:14<210:58:27, 479.49s/it]index: 0 loss: 1.3618944
index: 200 loss: 1.5043658
index: 400 loss: 1.3728101
index: 600 loss: 1.4492967
index: 800 loss: 1.578508
epoch: 16 validateAccuracy: 0.3379 trainAccuracy: 0.6875 delta: -0.0019
Training:   1%|          | 17/1600 [2:18:10<210:19:25, 478.31s/it]index: 0 loss: 1.4098153
index: 200 loss: 1.5279182
index: 400 loss: 1.565963
index: 600 loss: 1.4292505
index: 800 loss: 1.5094944
epoch: 17 validateAccuracy: 0.3369 trainAccuracy: 0.6862 delta: -0.0029
Training:   1%|          | 18/1600 [2:26:05<209:50:00, 477.50s/it]index: 0 loss: 1.4794042
index: 200 loss: 1.6185111
index: 400 loss: 1.5197935
index: 600 loss: 1.5201576
index: 800 loss: 1.5802153
epoch: 18 validateAccuracy: 0.3404 trainAccuracy: 0.6701 delta: 0.0006
Training:   1%|          | 19/1600 [2:34:06<210:05:39, 478.39s/it]index: 0 loss: 1.4849563
index: 200 loss: 1.6782249
index: 400 loss: 1.4659618
index: 600 loss: 1.6426024
index: 800 loss: 1.5784665
epoch: 19 validateAccuracy: 0.333 trainAccuracy: 0.6515 delta: -0.0074
Training:   1%|▏         | 20/1600 [2:42:05<210:05:43, 478.70s/it]index: 0 loss: 1.4218864
index: 200 loss: 1.3977225
index: 400 loss: 1.5631849
index: 600 loss: 1.4595829
index: 800 loss: 1.609111
epoch: 20 validateAccuracy: 0.3284 trainAccuracy: 0.625 delta: -0.012
Training:   1%|▏         | 21/1600 [2:50:05<210:06:33, 479.03s/it]index: 0 loss: 1.5572599
index: 200 loss: 1.6453552
index: 400 loss: 1.6507084
index: 600 loss: 1.3791767
index: 800 loss: 1.5647666
epoch: 21 validateAccuracy: 0.317 trainAccuracy: 0.6049 delta: -0.0234
Training:   1%|▏         | 22/1600 [2:58:06<210:11:30, 479.53s/it]index: 0 loss: 1.2952714
index: 200 loss: 1.6669564
index: 400 loss: 1.7482785
index: 600 loss: 1.651667
index: 800 loss: 1.4486158
epoch: 22 validateAccuracy: 0.3167 trainAccuracy: 0.5827 delta: -0.0237
Training:   1%|▏         | 23/1600 [3:06:06<210:09:25, 479.75s/it]index: 0 loss: 1.3372582
index: 200 loss: 1.7371173
index: 400 loss: 1.6870492
index: 600 loss: 1.9108129
index: 800 loss: 1.7686336
epoch: 23 validateAccuracy: 0.3202 trainAccuracy: 0.5631 delta: -0.0202
Training:   2%|▏         | 24/1600 [3:14:03<209:39:40, 478.92s/it]index: 0 loss: 1.6603726
index: 200 loss: 1.6725425
index: 400 loss: 1.6537819
index: 600 loss: 1.3039806
index: 800 loss: 1.4530851
epoch: 24 validateAccuracy: 0.3014 trainAccuracy: 0.5481 delta: -0.039
Training:   2%|▏         | 25/1600 [3:22:03<209:42:12, 479.32s/it]index: 0 loss: 1.6037095
index: 200 loss: 1.7884172
index: 400 loss: 2.0149016
index: 600 loss: 1.4868538
index: 800 loss: 1.7656578
epoch: 25 validateAccuracy: 0.304 trainAccuracy: 0.5391 delta: -0.0364
Training:   2%|▏         | 26/1600 [3:30:01<209:24:18, 478.94s/it]index: 0 loss: 1.7478622
index: 200 loss: 1.5013019
index: 400 loss: 1.825169
index: 600 loss: 1.5892681
index: 800 loss: 1.6903479
epoch: 26 validateAccuracy: 0.2894 trainAccuracy: 0.534 delta: -0.051
Training:   2%|▏         | 27/1600 [3:38:02<209:33:27, 479.60s/it]index: 0 loss: 1.4065585
index: 200 loss: 1.9475433
index: 400 loss: 1.4211236
index: 600 loss: 1.5217074
index: 800 loss: 2.0206861
epoch: 27 validateAccuracy: 0.2822 trainAccuracy: 0.5319 delta: -0.0582
Training:   2%|▏         | 28/1600 [3:46:00<209:11:36, 479.07s/it]index: 0 loss: 1.5813283
index: 200 loss: 1.6982315
index: 400 loss: 1.6913756
index: 600 loss: 1.7622468
index: 800 loss: 1.5383759
epoch: 28 validateAccuracy: 0.2685 trainAccuracy: 0.5347 delta: -0.0719
Training:   2%|▏         | 29/1600 [3:53:59<208:59:10, 478.90s/it]index: 0 loss: 1.6063198
index: 200 loss: 1.5457374
index: 400 loss: 1.746282
index: 600 loss: 1.8626008
index: 800 loss: 1.7543957
epoch: 29 validateAccuracy: 0.2961 trainAccuracy: 0.5415 delta: -0.0443
Training:   2%|▏         | 30/1600 [4:02:00<209:08:01, 479.54s/it]index: 0 loss: 1.3320282
index: 200 loss: 1.6928431
index: 400 loss: 1.8379987
index: 600 loss: 1.4529234
index: 800 loss: 1.5150826
epoch: 30 validateAccuracy: 0.2939 trainAccuracy: 0.55 delta: -0.0465
Training:   2%|▏         | 31/1600 [4:09:58<208:48:04, 479.09s/it]index: 0 loss: 1.6619734
index: 200 loss: 1.4883528
index: 400 loss: 1.3821843
index: 600 loss: 1.262189
index: 800 loss: 1.5057386
epoch: 31 validateAccuracy: 0.3022 trainAccuracy: 0.5605 delta: -0.0382
Training:   2%|▏         | 32/1600 [4:17:55<208:29:34, 478.68s/it]index: 0 loss: 1.3291545
index: 200 loss: 1.4453787
index: 400 loss: 1.5763423
index: 600 loss: 1.4505591
index: 800 loss: 1.5108038
epoch: 32 validateAccuracy: 0.3252 trainAccuracy: 0.5761 delta: -0.0152
Training:   2%|▏         | 33/1600 [4:25:54<208:18:30, 478.56s/it]index: 0 loss: 1.3476871
index: 200 loss: 1.5473319
index: 400 loss: 1.3464125
index: 600 loss: 1.3995235
index: 800 loss: 1.6533806
epoch: 33 validateAccuracy: 0.3155 trainAccuracy: 0.5921 delta: -0.0249
Training:   2%|▏         | 34/1600 [4:33:50<207:55:42, 478.00s/it]index: 0 loss: 1.2759999
index: 200 loss: 1.2296122
index: 400 loss: 1.4151429
index: 600 loss: 1.2831739
index: 800 loss: 1.4275687
epoch: 34 validateAccuracy: 0.3119 trainAccuracy: 0.613 delta: -0.0285
Training:   2%|▏         | 35/1600 [4:41:47<207:40:52, 477.73s/it]index: 0 loss: 1.0146344
index: 200 loss: 1.3503379
index: 400 loss: 1.5430613
index: 600 loss: 1.3223218
index: 800 loss: 1.3040894
epoch: 35 validateAccuracy: 0.3393 trainAccuracy: 0.6308 delta: -0.0011
Training:   2%|▏         | 36/1600 [4:49:44<207:26:13, 477.48s/it]index: 0 loss: 1.2824603
index: 200 loss: 1.0601437
index: 400 loss: 1.2186548
index: 600 loss: 1.2162395
index: 800 loss: 1.1794502
epoch: 36 validateAccuracy: 0.3238 trainAccuracy: 0.6563 delta: -0.0166
Training:   2%|▏         | 37/1600 [4:57:41<207:15:46, 477.38s/it]index: 0 loss: 1.0661598
index: 200 loss: 1.3436108
index: 400 loss: 1.034974
index: 600 loss: 1.1022938
index: 800 loss: 1.0032903
epoch: 37 validateAccuracy: 0.3356 trainAccuracy: 0.6843 delta: -0.0048
Training:   2%|▏         | 38/1600 [5:05:40<207:18:44, 477.80s/it]index: 0 loss: 1.1708041
index: 200 loss: 0.9993753
index: 400 loss: 1.0587223
index: 600 loss: 0.8075177
index: 800 loss: 1.0619676
epoch: 38 validateAccuracy: 0.3562 trainAccuracy: 0.7048 delta: 0.0158
Training:   2%|▏         | 39/1600 [5:13:39<207:20:23, 478.17s/it]index: 0 loss: 1.2240561
index: 200 loss: 0.8861054
index: 400 loss: 1.0435166
index: 600 loss: 0.82091063
index: 800 loss: 1.0177225
epoch: 39 validateAccuracy: 0.3586 trainAccuracy: 0.7332 delta: 0.0024
Training:   2%|▎         | 40/1600 [5:21:36<207:04:42, 477.87s/it]index: 0 loss: 1.1125395
index: 200 loss: 0.9656755
index: 400 loss: 1.0303946
index: 600 loss: 0.8211723
index: 800 loss: 0.9880519
epoch: 40 validateAccuracy: 0.3464 trainAccuracy: 0.7639 delta: -0.0122
Training:   3%|▎         | 41/1600 [5:29:32<206:40:13, 477.24s/it]index: 0 loss: 0.79337704
index: 200 loss: 0.745448
index: 400 loss: 0.8880575
index: 600 loss: 0.85180235
index: 800 loss: 0.7911749
epoch: 41 validateAccuracy: 0.3675 trainAccuracy: 0.7909 delta: 0.0089
Training:   3%|▎         | 42/1600 [5:37:32<206:55:20, 478.13s/it]index: 0 loss: 0.7361215
index: 200 loss: 0.7026449
index: 400 loss: 0.65740013
index: 600 loss: 0.78760254
index: 800 loss: 0.7200253
epoch: 42 validateAccuracy: 0.3694 trainAccuracy: 0.8155 delta: 0.0019
Training:   3%|▎         | 43/1600 [5:45:30<206:44:25, 478.01s/it]index: 0 loss: 0.83196664
index: 200 loss: 0.77120036
index: 400 loss: 0.59669566
index: 600 loss: 0.5971419
index: 800 loss: 0.76162803
epoch: 43 validateAccuracy: 0.3808 trainAccuracy: 0.8362 delta: 0.0114
Training:   3%|▎         | 44/1600 [5:53:33<207:15:09, 479.50s/it]index: 0 loss: 0.6054004
index: 200 loss: 0.74409777
index: 400 loss: 0.60800904
index: 600 loss: 0.6560119
index: 800 loss: 0.7312208
epoch: 44 validateAccuracy: 0.3797 trainAccuracy: 0.8554 delta: -0.0011
Training:   3%|▎         | 45/1600 [6:01:29<206:37:35, 478.36s/it]index: 0 loss: 0.5908379
index: 200 loss: 0.6239673
index: 400 loss: 0.95189726
index: 600 loss: 0.6694081
index: 800 loss: 0.57003355
epoch: 45 validateAccuracy: 0.3812 trainAccuracy: 0.8686 delta: 0.0004
Training:   3%|▎         | 46/1600 [6:09:26<206:20:05, 478.00s/it]index: 0 loss: 0.6479915
index: 200 loss: 0.5499323
index: 400 loss: 0.69358027
index: 600 loss: 0.5817968
index: 800 loss: 0.7295522
epoch: 46 validateAccuracy: 0.382 trainAccuracy: 0.8694 delta: 0.0008
Training:   3%|▎         | 47/1600 [6:17:25<206:18:08, 478.23s/it]index: 0 loss: 0.5778752
index: 200 loss: 0.56147724
index: 400 loss: 0.82439554
index: 600 loss: 0.61652213
index: 800 loss: 0.6851318
epoch: 47 validateAccuracy: 0.3801 trainAccuracy: 0.867 delta: -0.0019
Training:   3%|▎         | 48/1600 [6:25:23<206:06:24, 478.08s/it]index: 0 loss: 0.80808455
index: 200 loss: 0.5663712
index: 400 loss: 0.6476681
index: 600 loss: 0.6808759
index: 800 loss: 0.7471154
epoch: 48 validateAccuracy: 0.3801 trainAccuracy: 0.8614 delta: -0.0019
Training:   3%|▎         | 49/1600 [6:33:18<205:35:30, 477.20s/it]index: 0 loss: 0.69956774
index: 200 loss: 0.582978
index: 400 loss: 0.6904521
index: 600 loss: 0.7124305
index: 800 loss: 0.70802283
epoch: 49 validateAccuracy: 0.3811 trainAccuracy: 0.8462 delta: -0.0009
Training:   3%|▎         | 50/1600 [6:41:13<205:11:45, 476.58s/it]index: 0 loss: 0.708099
index: 200 loss: 0.7729332
index: 400 loss: 0.63409114
index: 600 loss: 0.62755406
index: 800 loss: 0.71816254
epoch: 50 validateAccuracy: 0.374 trainAccuracy: 0.8247 delta: -0.008
Training:   3%|▎         | 51/1600 [6:49:08<204:52:23, 476.14s/it]index: 0 loss: 0.77967954
index: 200 loss: 0.7370391
index: 400 loss: 0.9170451
index: 600 loss: 0.7520377
index: 800 loss: 0.71479607
epoch: 51 validateAccuracy: 0.3667 trainAccuracy: 0.8013 delta: -0.0153
Training:   3%|▎         | 52/1600 [6:57:04<204:40:17, 475.98s/it]index: 0 loss: 0.80179197
index: 200 loss: 1.0048223
index: 400 loss: 0.8881761
index: 600 loss: 0.9357422
index: 800 loss: 0.8457049
epoch: 52 validateAccuracy: 0.3665 trainAccuracy: 0.7792 delta: -0.0155
Training:   3%|▎         | 53/1600 [7:04:59<204:28:28, 475.83s/it]index: 0 loss: 1.0390651
index: 200 loss: 0.9831429
index: 400 loss: 0.9023302
index: 600 loss: 0.9146048
index: 800 loss: 0.93886125
epoch: 53 validateAccuracy: 0.3613 trainAccuracy: 0.7545 delta: -0.0207
Training:   3%|▎         | 54/1600 [7:12:55<204:24:59, 476.00s/it]index: 0 loss: 0.81898314
index: 200 loss: 0.8104865
index: 400 loss: 1.1245466
index: 600 loss: 0.92211145
index: 800 loss: 0.88819444
epoch: 54 validateAccuracy: 0.3545 trainAccuracy: 0.7345 delta: -0.0275
Training:   3%|▎         | 55/1600 [7:20:52<204:21:16, 476.17s/it]index: 0 loss: 0.792276
index: 200 loss: 0.96998626
index: 400 loss: 1.0765179
index: 600 loss: 1.0350183
index: 800 loss: 0.83203375
epoch: 55 validateAccuracy: 0.3447 trainAccuracy: 0.7175 delta: -0.0373
Training:   4%|▎         | 56/1600 [7:28:49<204:19:23, 476.40s/it]index: 0 loss: 1.144489
index: 200 loss: 0.9907812
index: 400 loss: 0.9941852
index: 600 loss: 1.0204352
index: 800 loss: 1.2382592
epoch: 56 validateAccuracy: 0.3407 trainAccuracy: 0.705 delta: -0.0413
Training:   4%|▎         | 57/1600 [7:36:45<204:06:03, 476.19s/it]index: 0 loss: 0.87218654
index: 200 loss: 1.2452669
index: 400 loss: 1.0667193
index: 600 loss: 1.2673337
index: 800 loss: 1.217287
epoch: 57 validateAccuracy: 0.3363 trainAccuracy: 0.6906 delta: -0.0457
Training:   4%|▎         | 58/1600 [7:44:40<203:50:58, 475.91s/it]index: 0 loss: 1.0226614
index: 200 loss: 1.1528407
index: 400 loss: 1.3280132
index: 600 loss: 1.0152236
index: 800 loss: 1.0950675
epoch: 58 validateAccuracy: 0.3465 trainAccuracy: 0.6858 delta: -0.0355
Training:   4%|▎         | 59/1600 [7:52:35<203:38:07, 475.72s/it]index: 0 loss: 0.71834135
index: 200 loss: 1.1056918
index: 400 loss: 0.975689
index: 600 loss: 0.96273494
index: 800 loss: 1.0001369
epoch: 59 validateAccuracy: 0.3281 trainAccuracy: 0.6862 delta: -0.0539
Training:   4%|▍         | 60/1600 [8:00:31<203:32:15, 475.80s/it]index: 0 loss: 1.0169854
index: 200 loss: 0.96624017
index: 400 loss: 1.3047746
index: 600 loss: 1.1551441
index: 800 loss: 1.025999
epoch: 60 validateAccuracy: 0.3381 trainAccuracy: 0.6818 delta: -0.0439
Training:   4%|▍         | 61/1600 [8:08:26<203:19:44, 475.62s/it]index: 0 loss: 1.0371501
index: 200 loss: 1.213039
index: 400 loss: 1.157016
index: 600 loss: 0.8906209
index: 800 loss: 1.1626282
epoch: 61 validateAccuracy: 0.3385 trainAccuracy: 0.6914 delta: -0.0435
Training:   4%|▍         | 62/1600 [8:16:24<203:29:01, 476.30s/it]index: 0 loss: 1.1719221
index: 200 loss: 0.9881262
index: 400 loss: 1.093561
index: 600 loss: 1.2445297
index: 800 loss: 1.180191
epoch: 62 validateAccuracy: 0.3529 trainAccuracy: 0.699 delta: -0.0291
Training:   4%|▍         | 63/1600 [8:24:23<203:36:40, 476.90s/it]index: 0 loss: 0.96230376
index: 200 loss: 0.8907956
index: 400 loss: 0.9607125
index: 600 loss: 0.9722032
index: 800 loss: 1.09498
epoch: 63 validateAccuracy: 0.3448 trainAccuracy: 0.7072 delta: -0.0372
Training:   4%|▍         | 64/1600 [8:32:21<203:43:21, 477.48s/it]index: 0 loss: 0.9831898
index: 200 loss: 1.138111
index: 400 loss: 0.96188307
index: 600 loss: 1.0219617
index: 800 loss: 0.81014705
epoch: 64 validateAccuracy: 0.3362 trainAccuracy: 0.7237 delta: -0.0458
Training:   4%|▍         | 65/1600 [8:40:20<203:43:00, 477.77s/it]index: 0 loss: 0.8006369
index: 200 loss: 0.8574304
index: 400 loss: 0.8654056
index: 600 loss: 0.89612
index: 800 loss: 0.9085806
epoch: 65 validateAccuracy: 0.3496 trainAccuracy: 0.7382 delta: -0.0324
Training:   4%|▍         | 66/1600 [8:48:20<203:51:53, 478.43s/it]index: 0 loss: 0.7859397
index: 200 loss: 0.7771163
index: 400 loss: 0.94324905
index: 600 loss: 0.76272416
index: 800 loss: 1.0736591
epoch: 66 validateAccuracy: 0.3557 trainAccuracy: 0.7553 delta: -0.0263
Training:   4%|▍         | 67/1600 [8:56:20<203:57:01, 478.94s/it]index: 0 loss: 0.7040607
index: 200 loss: 0.79798996
index: 400 loss: 0.8168637
index: 600 loss: 0.7471927
index: 800 loss: 0.7809517
epoch: 67 validateAccuracy: 0.3673 trainAccuracy: 0.7729 delta: -0.0147
Training:   4%|▍         | 68/1600 [9:04:19<203:52:01, 479.06s/it]index: 0 loss: 0.9537042
index: 200 loss: 0.72124153
index: 400 loss: 0.75074255
index: 600 loss: 0.74141645
index: 800 loss: 0.86634284
epoch: 68 validateAccuracy: 0.3755 trainAccuracy: 0.7974 delta: -0.0065
Training:   4%|▍         | 69/1600 [9:12:19<203:51:43, 479.36s/it]index: 0 loss: 0.62504286
index: 200 loss: 0.59114385
index: 400 loss: 0.8686148
index: 600 loss: 0.6788733
index: 800 loss: 0.7881778
epoch: 69 validateAccuracy: 0.3754 trainAccuracy: 0.8186 delta: -0.0066
Training:   4%|▍         | 70/1600 [9:20:18<203:41:18, 479.27s/it]index: 0 loss: 0.7355154
index: 200 loss: 0.76003456
index: 400 loss: 0.6556365
index: 600 loss: 0.57487357
index: 800 loss: 0.64945275
epoch: 70 validateAccuracy: 0.3818 trainAccuracy: 0.8388 delta: -0.0002
Training:   4%|▍         | 71/1600 [9:28:16<203:24:40, 478.93s/it]index: 0 loss: 0.6480608
index: 200 loss: 0.64674366
index: 400 loss: 0.4905233
index: 600 loss: 0.63852656
index: 800 loss: 0.6485703
epoch: 71 validateAccuracy: 0.3836 trainAccuracy: 0.8599 delta: 0.0016
Training:   4%|▍         | 72/1600 [9:36:19<203:43:35, 479.98s/it]index: 0 loss: 0.41131756
index: 200 loss: 0.51710933
index: 400 loss: 0.3546013
index: 600 loss: 0.55721915
index: 800 loss: 0.49565572
epoch: 72 validateAccuracy: 0.3902 trainAccuracy: 0.8781 delta: 0.0066
Training:   5%|▍         | 73/1600 [9:44:18<203:28:34, 479.71s/it]index: 0 loss: 0.48972347
index: 200 loss: 0.48278165
index: 400 loss: 0.3640357
index: 600 loss: 0.57600963
index: 800 loss: 0.39117298
epoch: 73 validateAccuracy: 0.3907 trainAccuracy: 0.8964 delta: 0.0005
Training:   5%|▍         | 74/1600 [9:52:18<203:22:00, 479.76s/it]index: 0 loss: 0.38280383
index: 200 loss: 0.49024805
index: 400 loss: 0.4437917
index: 600 loss: 0.49947965
index: 800 loss: 0.40909302
epoch: 74 validateAccuracy: 0.3908 trainAccuracy: 0.9108 delta: 0.0001
Training:   5%|▍         | 75/1600 [10:00:20<203:33:47, 480.54s/it]index: 0 loss: 0.47713494
index: 200 loss: 0.4612087
index: 400 loss: 0.41334724
index: 600 loss: 0.491072
index: 800 loss: 0.37854096
epoch: 75 validateAccuracy: 0.3921 trainAccuracy: 0.9161 delta: 0.0013
Training:   5%|▍         | 76/1600 [10:08:19<203:15:03, 480.12s/it]index: 0 loss: 0.530587
index: 200 loss: 0.4882756
index: 400 loss: 0.5459323
index: 600 loss: 0.38931233
index: 800 loss: 0.47323415
epoch: 76 validateAccuracy: 0.3902 trainAccuracy: 0.9187 delta: -0.0019
Training:   5%|▍         | 77/1600 [10:16:17<202:48:52, 479.40s/it]index: 0 loss: 0.40285605
index: 200 loss: 0.4856395
index: 400 loss: 0.50599414
index: 600 loss: 0.4592769
index: 800 loss: 0.41811
epoch: 77 validateAccuracy: 0.3889 trainAccuracy: 0.9168 delta: -0.0032
Training:   5%|▍         | 78/1600 [10:24:15<202:29:04, 478.94s/it]index: 0 loss: 0.49148387
index: 200 loss: 0.3698977
index: 400 loss: 0.4902034
index: 600 loss: 0.52037096
index: 800 loss: 0.7068904
epoch: 78 validateAccuracy: 0.3884 trainAccuracy: 0.9102 delta: -0.0037
Training:   5%|▍         | 79/1600 [10:32:10<201:53:05, 477.83s/it]index: 0 loss: 0.43019143
index: 200 loss: 0.47011074
index: 400 loss: 0.43474764
index: 600 loss: 0.5591067
index: 800 loss: 0.42929634
epoch: 79 validateAccuracy: 0.3876 trainAccuracy: 0.9001 delta: -0.0045
Training:   5%|▌         | 80/1600 [10:40:07<201:33:18, 477.37s/it]index: 0 loss: 0.53101504
index: 200 loss: 0.5436841
index: 400 loss: 0.5674081
index: 600 loss: 0.6228957
index: 800 loss: 0.58049655
epoch: 80 validateAccuracy: 0.3913 trainAccuracy: 0.886 delta: -0.0008
Training:   5%|▌         | 81/1600 [10:48:01<201:06:55, 476.64s/it]index: 0 loss: 0.42600065
index: 200 loss: 0.5049912
index: 400 loss: 0.48526242
index: 600 loss: 0.61069024
index: 800 loss: 0.59718186
epoch: 81 validateAccuracy: 0.3786 trainAccuracy: 0.8638 delta: -0.0135
Training:   5%|▌         | 82/1600 [10:55:57<200:53:43, 476.43s/it]index: 0 loss: 0.442906
index: 200 loss: 0.63143057
index: 400 loss: 0.54391325
index: 600 loss: 0.51931185
index: 800 loss: 0.4975738
epoch: 82 validateAccuracy: 0.375 trainAccuracy: 0.8456 delta: -0.0171
Training:   5%|▌         | 83/1600 [11:03:54<200:45:55, 476.44s/it]index: 0 loss: 0.59228617
index: 200 loss: 0.5876609
index: 400 loss: 0.7530618
index: 600 loss: 0.6630353
index: 800 loss: 0.74438393
epoch: 83 validateAccuracy: 0.3699 trainAccuracy: 0.8257 delta: -0.0222
Training:   5%|▌         | 84/1600 [11:11:49<200:25:39, 475.95s/it]index: 0 loss: 0.50538623
index: 200 loss: 0.6477725
index: 400 loss: 0.7642758
index: 600 loss: 0.7414135
index: 800 loss: 0.72338814
epoch: 84 validateAccuracy: 0.3572 trainAccuracy: 0.8016 delta: -0.0349
Training:   5%|▌         | 85/1600 [11:19:46<200:28:37, 476.38s/it]index: 0 loss: 0.57784647
index: 200 loss: 0.6784369
index: 400 loss: 0.8689712
index: 600 loss: 0.64808935
index: 800 loss: 0.7269337
epoch: 85 validateAccuracy: 0.3641 trainAccuracy: 0.7852 delta: -0.028
Training:   5%|▌         | 86/1600 [11:27:43<200:21:18, 476.41s/it]index: 0 loss: 0.7227461
index: 200 loss: 0.81806326
index: 400 loss: 0.8711184
index: 600 loss: 0.86076814
index: 800 loss: 0.9107075
epoch: 86 validateAccuracy: 0.3573 trainAccuracy: 0.7733 delta: -0.0348
Training:   5%|▌         | 87/1600 [11:35:43<200:42:39, 477.57s/it]index: 0 loss: 0.70234996
index: 200 loss: 0.85923684
index: 400 loss: 0.747211
index: 600 loss: 0.7349253
index: 800 loss: 0.7541203
epoch: 87 validateAccuracy: 0.3595 trainAccuracy: 0.7583 delta: -0.0326
Training:   6%|▌         | 88/1600 [11:43:42<200:48:39, 478.12s/it]index: 0 loss: 0.76351047
index: 200 loss: 1.0008065
index: 400 loss: 0.8760145
index: 600 loss: 0.8264322
index: 800 loss: 0.86961544
epoch: 88 validateAccuracy: 0.3456 trainAccuracy: 0.7509 delta: -0.0465
Training:   6%|▌         | 89/1600 [11:51:40<200:37:48, 478.01s/it]index: 0 loss: 0.69320476
index: 200 loss: 0.8664321
index: 400 loss: 1.021187
index: 600 loss: 0.964179
index: 800 loss: 0.9300339
epoch: 89 validateAccuracy: 0.3668 trainAccuracy: 0.7486 delta: -0.0253
Training:   6%|▌         | 90/1600 [11:59:39<200:39:41, 478.40s/it]index: 0 loss: 0.757441
index: 200 loss: 1.1468179
index: 400 loss: 0.8465081
index: 600 loss: 0.88782364
index: 800 loss: 0.9353644
epoch: 90 validateAccuracy: 0.3579 trainAccuracy: 0.7466 delta: -0.0342
Training:   6%|▌         | 91/1600 [12:07:39<200:43:14, 478.86s/it]index: 0 loss: 0.9538754
index: 200 loss: 0.9433859
index: 400 loss: 0.70090884
index: 600 loss: 0.9825145
index: 800 loss: 0.7129767
epoch: 91 validateAccuracy: 0.3534 trainAccuracy: 0.7471 delta: -0.0387
Training:   6%|▌         | 92/1600 [12:15:39<200:44:42, 479.23s/it]index: 0 loss: 0.7697061
index: 200 loss: 0.82096654
index: 400 loss: 0.6969497
index: 600 loss: 0.75504833
index: 800 loss: 0.6762253
epoch: 92 validateAccuracy: 0.3542 trainAccuracy: 0.7539 delta: -0.0379
Training:   6%|▌         | 93/1600 [12:23:40<200:45:53, 479.60s/it]index: 0 loss: 0.82308936
index: 200 loss: 0.89285827
index: 400 loss: 0.76055497
index: 600 loss: 0.88336164
index: 800 loss: 1.015519
epoch: 93 validateAccuracy: 0.3585 trainAccuracy: 0.7629 delta: -0.0336
Training:   6%|▌         | 94/1600 [12:31:40<200:40:20, 479.69s/it]index: 0 loss: 0.668577
index: 200 loss: 0.72330606
index: 400 loss: 0.6068384
index: 600 loss: 0.8590322
index: 800 loss: 0.7024451
epoch: 94 validateAccuracy: 0.3555 trainAccuracy: 0.7752 delta: -0.0366
Training:   6%|▌         | 95/1600 [12:39:40<200:38:24, 479.94s/it]index: 0 loss: 0.7801777
index: 200 loss: 0.82698596
index: 400 loss: 0.71457356
index: 600 loss: 0.76518947
index: 800 loss: 0.7247672
epoch: 95 validateAccuracy: 0.3614 trainAccuracy: 0.7879 delta: -0.0307
Training:   6%|▌         | 96/1600 [12:47:41<200:34:16, 480.09s/it]index: 0 loss: 0.7817248
index: 200 loss: 0.8135881
index: 400 loss: 0.645541
index: 600 loss: 0.69835436
index: 800 loss: 0.60519636
epoch: 96 validateAccuracy: 0.367 trainAccuracy: 0.8019 delta: -0.0251
Training:   6%|▌         | 97/1600 [12:55:38<200:06:28, 479.30s/it]index: 0 loss: 0.7999819
index: 200 loss: 0.7229543
index: 400 loss: 0.5823897
index: 600 loss: 0.55715257
index: 800 loss: 0.7408737
epoch: 97 validateAccuracy: 0.3714 trainAccuracy: 0.8201 delta: -0.0207
Training:   6%|▌         | 98/1600 [13:03:39<200:08:06, 479.68s/it]index: 0 loss: 0.46622816
index: 200 loss: 0.7356083
index: 400 loss: 0.6346498
index: 600 loss: 0.6062311
index: 800 loss: 0.83478904
epoch: 98 validateAccuracy: 0.3825 trainAccuracy: 0.8391 delta: -0.0096
Training:   6%|▌         | 99/1600 [13:11:38<200:00:25, 479.70s/it]index: 0 loss: 0.5287287
index: 200 loss: 0.44049445
index: 400 loss: 0.54423815
index: 600 loss: 0.46511215
index: 800 loss: 0.59055376
epoch: 99 validateAccuracy: 0.3788 trainAccuracy: 0.8554 delta: -0.0133
Training:   6%|▋         | 100/1600 [13:19:37<199:44:21, 479.37s/it]index: 0 loss: 0.6002713
index: 200 loss: 0.5412999
index: 400 loss: 0.4455935
index: 600 loss: 0.4564066
index: 800 loss: 0.58743525
epoch: 100 validateAccuracy: 0.3812 trainAccuracy: 0.8728 delta: -0.0109
Training:   6%|▋         | 101/1600 [13:27:33<199:08:52, 478.27s/it]index: 0 loss: 0.41750723
index: 200 loss: 0.4025419
index: 400 loss: 0.37899533
index: 600 loss: 0.45642638
index: 800 loss: 0.52004683
epoch: 101 validateAccuracy: 0.3792 trainAccuracy: 0.8915 delta: -0.0129
Training:   6%|▋         | 102/1600 [13:35:30<198:56:20, 478.09s/it]index: 0 loss: 0.37213188
index: 200 loss: 0.39361754
index: 400 loss: 0.37661427
index: 600 loss: 0.47491264
index: 800 loss: 0.5083028
epoch: 102 validateAccuracy: 0.3871 trainAccuracy: 0.9091 delta: -0.005
Training:   6%|▋         | 103/1600 [13:43:29<198:56:04, 478.40s/it]index: 0 loss: 0.34395137
index: 200 loss: 0.3575824
index: 400 loss: 0.4441933
index: 600 loss: 0.5046296
index: 800 loss: 0.46867806
epoch: 103 validateAccuracy: 0.3844 trainAccuracy: 0.9201 delta: -0.0077
Training:   6%|▋         | 104/1600 [13:51:28<198:48:32, 478.42s/it]index: 0 loss: 0.4244613
index: 200 loss: 0.36591718
index: 400 loss: 0.3676546
index: 600 loss: 0.31048504
index: 800 loss: 0.425657
epoch: 104 validateAccuracy: 0.3917 trainAccuracy: 0.9307 delta: -0.0004
Training:   7%|▋         | 105/1600 [13:59:26<198:36:59, 478.27s/it]index: 0 loss: 0.3763678
index: 200 loss: 0.39207917
index: 400 loss: 0.3981673
index: 600 loss: 0.43635735
index: 800 loss: 0.4289224
epoch: 105 validateAccuracy: 0.3921 trainAccuracy: 0.9357 delta: 0.0
Training:   7%|▋         | 106/1600 [14:07:25<198:33:09, 478.44s/it]index: 0 loss: 0.3173894
index: 200 loss: 0.4217256
index: 400 loss: 0.41829532
index: 600 loss: 0.3111358
index: 800 loss: 0.32760474
epoch: 106 validateAccuracy: 0.39 trainAccuracy: 0.9391 delta: -0.0021
Training:   7%|▋         | 107/1600 [14:15:22<198:15:48, 478.06s/it]index: 0 loss: 0.33569092
index: 200 loss: 0.37940618
index: 400 loss: 0.29832894
index: 600 loss: 0.37976283
index: 800 loss: 0.39387196
epoch: 107 validateAccuracy: 0.392 trainAccuracy: 0.9402 delta: -0.0001
Training:   7%|▋         | 108/1600 [14:23:22<198:25:34, 478.78s/it]index: 0 loss: 0.31696427
index: 200 loss: 0.33086234
index: 400 loss: 0.3738996
index: 600 loss: 0.33059037
index: 800 loss: 0.34471723
epoch: 108 validateAccuracy: 0.3935 trainAccuracy: 0.9321 delta: 0.0014
Training:   7%|▋         | 109/1600 [14:31:24<198:40:34, 479.70s/it]index: 0 loss: 0.3650667
index: 200 loss: 0.35590175
index: 400 loss: 0.32958046
index: 600 loss: 0.39656618
index: 800 loss: 0.3420689
epoch: 109 validateAccuracy: 0.3904 trainAccuracy: 0.9254 delta: -0.0031
Training:   7%|▋         | 110/1600 [14:39:23<198:24:34, 479.38s/it]index: 0 loss: 0.35554096
index: 200 loss: 0.46147934
index: 400 loss: 0.38201323
index: 600 loss: 0.4267551
index: 800 loss: 0.37146237
epoch: 110 validateAccuracy: 0.3828 trainAccuracy: 0.9117 delta: -0.0107
Training:   7%|▋         | 111/1600 [14:47:22<198:11:52, 479.19s/it]index: 0 loss: 0.3122206
index: 200 loss: 0.5409983
index: 400 loss: 0.47216797
index: 600 loss: 0.60702366
index: 800 loss: 0.39396772
epoch: 111 validateAccuracy: 0.3885 trainAccuracy: 0.8944 delta: -0.005
Training:   7%|▋         | 112/1600 [14:55:21<198:02:06, 479.12s/it]index: 0 loss: 0.2838208
index: 200 loss: 0.48147327
index: 400 loss: 0.4388657
index: 600 loss: 0.44737315
index: 800 loss: 0.6458618
epoch: 112 validateAccuracy: 0.385 trainAccuracy: 0.8748 delta: -0.0085
Training:   7%|▋         | 113/1600 [15:03:20<197:57:09, 479.24s/it]index: 0 loss: 0.55444187
index: 200 loss: 0.51803267
index: 400 loss: 0.48916224
index: 600 loss: 0.46752265
index: 800 loss: 0.51198405
epoch: 113 validateAccuracy: 0.3728 trainAccuracy: 0.858 delta: -0.0207
Training:   7%|▋         | 114/1600 [15:11:18<197:42:02, 478.95s/it]index: 0 loss: 0.45020866
index: 200 loss: 0.5895633
index: 400 loss: 0.6429953
index: 600 loss: 0.62555385
index: 800 loss: 0.5720058
epoch: 114 validateAccuracy: 0.3667 trainAccuracy: 0.8354 delta: -0.0268
Training:   7%|▋         | 115/1600 [15:19:18<197:42:24, 479.29s/it]index: 0 loss: 0.59340054
index: 200 loss: 0.785682
index: 400 loss: 0.58682495
index: 600 loss: 0.7662272
index: 800 loss: 0.6843543
epoch: 115 validateAccuracy: 0.3659 trainAccuracy: 0.8173 delta: -0.0276
Training:   7%|▋         | 116/1600 [15:27:16<197:23:29, 478.85s/it]index: 0 loss: 0.46167836
index: 200 loss: 0.8136121
index: 400 loss: 0.49039173
index: 600 loss: 0.60128415
index: 800 loss: 0.58071077
epoch: 116 validateAccuracy: 0.3673 trainAccuracy: 0.8092 delta: -0.0262
Training:   7%|▋         | 117/1600 [15:35:15<197:14:17, 478.80s/it]index: 0 loss: 0.57162833
index: 200 loss: 0.6779666
index: 400 loss: 0.72387487
index: 600 loss: 0.63113946
index: 800 loss: 0.54779345
epoch: 117 validateAccuracy: 0.3614 trainAccuracy: 0.7974 delta: -0.0321
Training:   7%|▋         | 118/1600 [15:43:10<196:40:58, 477.77s/it]index: 0 loss: 0.72585076
index: 200 loss: 0.68748915
index: 400 loss: 0.82144165
index: 600 loss: 0.7135711
index: 800 loss: 0.80247146
epoch: 118 validateAccuracy: 0.3614 trainAccuracy: 0.7868 delta: -0.0321
Training:   7%|▋         | 119/1600 [15:51:06<196:14:20, 477.02s/it]index: 0 loss: 0.4987236
index: 200 loss: 0.7579143
index: 400 loss: 0.6178311
index: 600 loss: 0.65544814
index: 800 loss: 0.770397
epoch: 119 validateAccuracy: 0.363 trainAccuracy: 0.7853 delta: -0.0305
Training:   8%|▊         | 120/1600 [15:59:03<196:11:42, 477.23s/it]index: 0 loss: 0.5722678
index: 200 loss: 0.768401
index: 400 loss: 0.970326
index: 600 loss: 0.7358683
index: 800 loss: 0.87806755
epoch: 120 validateAccuracy: 0.3641 trainAccuracy: 0.7823 delta: -0.0294
Training:   8%|▊         | 121/1600 [16:06:59<195:55:19, 476.89s/it]index: 0 loss: 0.62613595
index: 200 loss: 0.8385048
index: 400 loss: 0.7497363
index: 600 loss: 0.8263669
index: 800 loss: 1.0182471
epoch: 121 validateAccuracy: 0.3596 trainAccuracy: 0.7859 delta: -0.0339
Training:   8%|▊         | 122/1600 [16:14:59<196:09:53, 477.80s/it]index: 0 loss: 0.82086223
index: 200 loss: 0.7903827
index: 400 loss: 0.72948855
index: 600 loss: 0.6057591
index: 800 loss: 0.71688884
epoch: 122 validateAccuracy: 0.364 trainAccuracy: 0.7893 delta: -0.0295
Training:   8%|▊         | 123/1600 [16:22:57<196:02:15, 477.82s/it]index: 0 loss: 0.73263204
index: 200 loss: 0.6524677
index: 400 loss: 0.74369395
index: 600 loss: 0.7992041
index: 800 loss: 0.81828684
epoch: 123 validateAccuracy: 0.3651 trainAccuracy: 0.7968 delta: -0.0284
Training:   8%|▊         | 124/1600 [16:30:55<195:56:03, 477.89s/it]index: 0 loss: 0.5530324
index: 200 loss: 0.67695296
index: 400 loss: 0.8203539
index: 600 loss: 0.606584
index: 800 loss: 0.63170123
epoch: 124 validateAccuracy: 0.374 trainAccuracy: 0.8085 delta: -0.0195
Training:   8%|▊         | 125/1600 [16:38:53<195:47:18, 477.86s/it]index: 0 loss: 0.5892367
index: 200 loss: 0.54936975
index: 400 loss: 0.85457253
index: 600 loss: 0.66555625
index: 800 loss: 0.759184
epoch: 125 validateAccuracy: 0.372 trainAccuracy: 0.8193 delta: -0.0215
Training:   8%|▊         | 126/1600 [16:46:54<196:00:41, 478.73s/it]index: 0 loss: 0.58036256
index: 200 loss: 0.58697194
index: 400 loss: 0.79079676
index: 600 loss: 0.4903456
index: 800 loss: 0.42714646
epoch: 126 validateAccuracy: 0.3743 trainAccuracy: 0.8258 delta: -0.0192
Training:   8%|▊         | 127/1600 [16:54:53<195:55:49, 478.85s/it]index: 0 loss: 0.548977
index: 200 loss: 0.52502525
index: 400 loss: 0.6203874
index: 600 loss: 0.56486964
index: 800 loss: 0.7354552
epoch: 127 validateAccuracy: 0.3771 trainAccuracy: 0.8427 delta: -0.0164
Training:   8%|▊         | 128/1600 [17:02:53<195:53:34, 479.09s/it]index: 0 loss: 0.3661256
index: 200 loss: 0.5392562
index: 400 loss: 0.535995
index: 600 loss: 0.51201123
index: 800 loss: 0.4121138
epoch: 128 validateAccuracy: 0.3844 trainAccuracy: 0.862 delta: -0.0091
Training:   8%|▊         | 129/1600 [17:10:51<195:41:19, 478.91s/it]index: 0 loss: 0.53058535
index: 200 loss: 0.5256356
index: 400 loss: 0.45535195
index: 600 loss: 0.41782102
index: 800 loss: 0.31076837
epoch: 129 validateAccuracy: 0.3865 trainAccuracy: 0.8786 delta: -0.007
Training:   8%|▊         | 130/1600 [17:18:49<195:24:24, 478.55s/it]index: 0 loss: 0.5099434
index: 200 loss: 0.4205692
index: 400 loss: 0.43110874
index: 600 loss: 0.4083797
index: 800 loss: 0.38055626
epoch: 130 validateAccuracy: 0.3842 trainAccuracy: 0.8939 delta: -0.0093
Training:   8%|▊         | 131/1600 [17:26:45<195:02:44, 477.99s/it]index: 0 loss: 0.4544879
index: 200 loss: 0.2850802
index: 400 loss: 0.42581373
index: 600 loss: 0.35736278
index: 800 loss: 0.40097067
epoch: 131 validateAccuracy: 0.3891 trainAccuracy: 0.9085 delta: -0.0044
Training:   8%|▊         | 132/1600 [17:34:43<194:49:07, 477.76s/it]index: 0 loss: 0.3467538
index: 200 loss: 0.32371697
index: 400 loss: 0.29887924
index: 600 loss: 0.33162144
index: 800 loss: 0.3882003
epoch: 132 validateAccuracy: 0.3954 trainAccuracy: 0.9216 delta: 0.0019
Training:   8%|▊         | 133/1600 [17:42:43<195:02:40, 478.64s/it]index: 0 loss: 0.29645294
index: 200 loss: 0.42766333
index: 400 loss: 0.35929698
index: 600 loss: 0.24468265
index: 800 loss: 0.38391456
epoch: 133 validateAccuracy: 0.3936 trainAccuracy: 0.9333 delta: -0.0018
Training:   8%|▊         | 134/1600 [17:50:41<194:47:44, 478.35s/it]index: 0 loss: 0.40963125
index: 200 loss: 0.303395
index: 400 loss: 0.30707997
index: 600 loss: 0.30384177
index: 800 loss: 0.27470583
epoch: 134 validateAccuracy: 0.3998 trainAccuracy: 0.9445 delta: 0.0044
Training:   8%|▊         | 135/1600 [17:58:41<194:55:24, 478.99s/it]index: 0 loss: 0.3225773
index: 200 loss: 0.25703907
index: 400 loss: 0.3573233
index: 600 loss: 0.29668146
index: 800 loss: 0.44655377
epoch: 135 validateAccuracy: 0.3944 trainAccuracy: 0.9469 delta: -0.0054
Training:   8%|▊         | 136/1600 [18:06:41<194:51:19, 479.15s/it]index: 0 loss: 0.28260994
index: 200 loss: 0.29531255
index: 400 loss: 0.34972635
index: 600 loss: 0.32522163
index: 800 loss: 0.28317115
epoch: 136 validateAccuracy: 0.3989 trainAccuracy: 0.9491 delta: -0.0009
Training:   9%|▊         | 137/1600 [18:14:39<194:34:16, 478.78s/it]index: 0 loss: 0.3711524
index: 200 loss: 0.3370388
index: 400 loss: 0.40636414
index: 600 loss: 0.35358968
index: 800 loss: 0.23358442
epoch: 137 validateAccuracy: 0.3988 trainAccuracy: 0.9482 delta: -0.001
Training:   9%|▊         | 138/1600 [18:22:38<194:27:59, 478.85s/it]index: 0 loss: 0.2432587
index: 200 loss: 0.30006215
index: 400 loss: 0.38497317
index: 600 loss: 0.30985564
index: 800 loss: 0.29225308
epoch: 138 validateAccuracy: 0.3953 trainAccuracy: 0.9458 delta: -0.0045
Training:   9%|▊         | 139/1600 [18:30:37<194:20:15, 478.86s/it]index: 0 loss: 0.4221169
index: 200 loss: 0.3864283
index: 400 loss: 0.32823563
index: 600 loss: 0.33813274
index: 800 loss: 0.31733897
epoch: 139 validateAccuracy: 0.395 trainAccuracy: 0.9381 delta: -0.0048
Training:   9%|▉         | 140/1600 [18:38:36<194:16:19, 479.03s/it]index: 0 loss: 0.3967974
index: 200 loss: 0.39209968
index: 400 loss: 0.40614703
index: 600 loss: 0.32498604
index: 800 loss: 0.3185903
epoch: 140 validateAccuracy: 0.3944 trainAccuracy: 0.9264 delta: -0.0054
Training:   9%|▉         | 141/1600 [18:46:37<194:21:49, 479.58s/it]index: 0 loss: 0.33297879
index: 200 loss: 0.43987662
index: 400 loss: 0.33839098
index: 600 loss: 0.3631443
index: 800 loss: 0.53445685
epoch: 141 validateAccuracy: 0.391 trainAccuracy: 0.9092 delta: -0.0088
Training:   9%|▉         | 142/1600 [18:54:35<194:01:20, 479.07s/it]index: 0 loss: 0.40825078
index: 200 loss: 0.4504667
index: 400 loss: 0.45920447
index: 600 loss: 0.4327316
index: 800 loss: 0.5237637
epoch: 142 validateAccuracy: 0.3807 trainAccuracy: 0.8956 delta: -0.0191
Training:   9%|▉         | 143/1600 [19:02:34<193:55:16, 479.15s/it]index: 0 loss: 0.31175342
index: 200 loss: 0.4699344
index: 400 loss: 0.44893858
index: 600 loss: 0.51600516
index: 800 loss: 0.46645188
epoch: 143 validateAccuracy: 0.3777 trainAccuracy: 0.8779 delta: -0.0221
Training:   9%|▉         | 144/1600 [19:10:33<193:42:41, 478.96s/it]index: 0 loss: 0.5420042
index: 200 loss: 0.5283027
index: 400 loss: 0.5985699
index: 600 loss: 0.4810942
index: 800 loss: 0.4685848
epoch: 144 validateAccuracy: 0.3777 trainAccuracy: 0.8589 delta: -0.0221
Training:   9%|▉         | 145/1600 [19:18:33<193:40:04, 479.18s/it]index: 0 loss: 0.48902094
index: 200 loss: 0.65574837
index: 400 loss: 0.50393236
index: 600 loss: 0.61716473
index: 800 loss: 0.5542201
epoch: 145 validateAccuracy: 0.3791 trainAccuracy: 0.8426 delta: -0.0207
Training:   9%|▉         | 146/1600 [19:26:33<193:40:14, 479.51s/it]index: 0 loss: 0.64244455
index: 200 loss: 0.5542644
index: 400 loss: 0.75723803
index: 600 loss: 0.6372791
index: 800 loss: 0.5814838
epoch: 146 validateAccuracy: 0.3697 trainAccuracy: 0.8338 delta: -0.0301
Training:   9%|▉         | 147/1600 [19:34:33<193:40:21, 479.85s/it]index: 0 loss: 0.44622993
index: 200 loss: 0.6504023
index: 400 loss: 0.6592315
index: 600 loss: 0.81055754
index: 800 loss: 0.6660551
epoch: 147 validateAccuracy: 0.3686 trainAccuracy: 0.8219 delta: -0.0312
Training:   9%|▉         | 148/1600 [19:42:31<193:19:02, 479.30s/it]index: 0 loss: 0.73720086
index: 200 loss: 0.6749532
index: 400 loss: 0.6627729
index: 600 loss: 0.789568
index: 800 loss: 0.56085294
epoch: 148 validateAccuracy: 0.3647 trainAccuracy: 0.8143 delta: -0.0351
Training:   9%|▉         | 149/1600 [19:50:31<193:16:15, 479.51s/it]index: 0 loss: 0.6479834
index: 200 loss: 0.5938777
index: 400 loss: 0.83209264
index: 600 loss: 0.66496015
index: 800 loss: 0.6896938
epoch: 149 validateAccuracy: 0.3742 trainAccuracy: 0.8089 delta: -0.0256
Training:   9%|▉         | 150/1600 [19:58:37<193:53:25, 481.38s/it]index: 0 loss: 0.6377471
index: 200 loss: 0.72170705
index: 400 loss: 0.7045967
index: 600 loss: 0.6180654
index: 800 loss: 0.7340634
epoch: 150 validateAccuracy: 0.3612 trainAccuracy: 0.8065 delta: -0.0386
Training:   9%|▉         | 151/1600 [20:06:34<193:13:19, 480.06s/it]index: 0 loss: 0.58204037
index: 200 loss: 0.6061475
index: 400 loss: 0.72466564
index: 600 loss: 0.6631724
index: 800 loss: 0.6239644
epoch: 151 validateAccuracy: 0.3662 trainAccuracy: 0.8076 delta: -0.0336
Training:  10%|▉         | 152/1600 [20:14:31<192:38:55, 478.96s/it]index: 0 loss: 0.5723748
index: 200 loss: 0.59538317
index: 400 loss: 0.6321847
index: 600 loss: 0.8025781
index: 800 loss: 0.7243099
epoch: 152 validateAccuracy: 0.3787 trainAccuracy: 0.8108 delta: -0.0211
Training:  10%|▉         | 153/1600 [20:22:29<192:26:24, 478.77s/it]index: 0 loss: 0.77954423
index: 200 loss: 0.84562963
index: 400 loss: 0.56602514
index: 600 loss: 0.5684174
index: 800 loss: 0.6848451
epoch: 153 validateAccuracy: 0.3759 trainAccuracy: 0.8198 delta: -0.0239
Training:  10%|▉         | 154/1600 [20:30:27<192:14:05, 478.59s/it]index: 0 loss: 0.6022079
index: 200 loss: 0.58068156
index: 400 loss: 0.6566913
index: 600 loss: 0.7164324
index: 800 loss: 0.6110148
epoch: 154 validateAccuracy: 0.3786 trainAccuracy: 0.828 delta: -0.0212
Training:  10%|▉         | 155/1600 [20:38:26<192:10:09, 478.76s/it]index: 0 loss: 0.571514
index: 200 loss: 0.7232719
index: 400 loss: 0.64468884
index: 600 loss: 0.5874134
index: 800 loss: 0.6573441
epoch: 155 validateAccuracy: 0.3736 trainAccuracy: 0.8357 delta: -0.0262
Training:  10%|▉         | 156/1600 [20:46:26<192:07:25, 478.98s/it]index: 0 loss: 0.4843052
index: 200 loss: 0.5445011
index: 400 loss: 0.57949126
index: 600 loss: 0.70544785
index: 800 loss: 0.5496591
epoch: 156 validateAccuracy: 0.3918 trainAccuracy: 0.851 delta: -0.008
Training:  10%|▉         | 157/1600 [20:54:23<191:49:55, 478.58s/it]index: 0 loss: 0.5160568
index: 200 loss: 0.37102678
index: 400 loss: 0.51118803
index: 600 loss: 0.45293355
index: 800 loss: 0.67613715
epoch: 157 validateAccuracy: 0.393 trainAccuracy: 0.8626 delta: -0.0068
Training:  10%|▉         | 158/1600 [21:02:21<191:38:25, 478.44s/it]index: 0 loss: 0.47983605
index: 200 loss: 0.453424
index: 400 loss: 0.38843316
index: 600 loss: 0.57170117
index: 800 loss: 0.44324377
epoch: 158 validateAccuracy: 0.3877 trainAccuracy: 0.8777 delta: -0.0121
Training:  10%|▉         | 159/1600 [21:10:20<191:29:46, 478.41s/it]index: 0 loss: 0.46563843
index: 200 loss: 0.43519977
index: 400 loss: 0.35478705
index: 600 loss: 0.3538064
index: 800 loss: 0.47310582
epoch: 159 validateAccuracy: 0.3936 trainAccuracy: 0.8935 delta: -0.0062
Training:  10%|█         | 160/1600 [21:18:18<191:21:44, 478.41s/it]index: 0 loss: 0.48174474
index: 200 loss: 0.30277646
index: 400 loss: 0.35151982
index: 600 loss: 0.4704811
index: 800 loss: 0.44416907
epoch: 160 validateAccuracy: 0.3898 trainAccuracy: 0.9082 delta: -0.01
Training:  10%|█         | 161/1600 [21:26:18<191:22:13, 478.76s/it]index: 0 loss: 0.41393012
index: 200 loss: 0.33795103
index: 400 loss: 0.24997707
index: 600 loss: 0.36079368
index: 800 loss: 0.31121436
epoch: 161 validateAccuracy: 0.3939 trainAccuracy: 0.9215 delta: -0.0059
Training:  10%|█         | 162/1600 [21:34:17<191:19:54, 478.99s/it]index: 0 loss: 0.32864335
index: 200 loss: 0.31652328
index: 400 loss: 0.25775167
index: 600 loss: 0.3640516
index: 800 loss: 0.32628933
epoch: 162 validateAccuracy: 0.4013 trainAccuracy: 0.9327 delta: 0.0015
Training:  10%|█         | 163/1600 [21:42:19<191:33:12, 479.88s/it]index: 0 loss: 0.45781776
index: 200 loss: 0.24379303
index: 400 loss: 0.3304448
index: 600 loss: 0.3007591
index: 800 loss: 0.31120163
epoch: 163 validateAccuracy: 0.401 trainAccuracy: 0.9422 delta: -0.0003
Training:  10%|█         | 164/1600 [21:50:16<191:05:12, 479.05s/it]index: 0 loss: 0.20601581
index: 200 loss: 0.32768288
index: 400 loss: 0.3651469
index: 600 loss: 0.24930593
index: 800 loss: 0.2047316
epoch: 164 validateAccuracy: 0.4001 trainAccuracy: 0.9496 delta: -0.0012
Training:  10%|█         | 165/1600 [21:58:15<190:51:32, 478.81s/it]index: 0 loss: 0.26125142
index: 200 loss: 0.25580662
index: 400 loss: 0.2951622
index: 600 loss: 0.3672653
index: 800 loss: 0.2614293
epoch: 165 validateAccuracy: 0.402 trainAccuracy: 0.9553 delta: 0.0007
Training:  10%|█         | 166/1600 [22:06:18<191:15:52, 480.16s/it]index: 0 loss: 0.26892895
index: 200 loss: 0.3188048
index: 400 loss: 0.19326301
index: 600 loss: 0.19881172
index: 800 loss: 0.22939977
epoch: 166 validateAccuracy: 0.4017 trainAccuracy: 0.9586 delta: -0.0003
Training:  10%|█         | 167/1600 [22:14:18<191:10:25, 480.27s/it]index: 0 loss: 0.3040288
index: 200 loss: 0.30165288
index: 400 loss: 0.24513265
index: 600 loss: 0.2791245
index: 800 loss: 0.3311613
epoch: 167 validateAccuracy: 0.4029 trainAccuracy: 0.9556 delta: 0.0009
Training:  10%|█         | 168/1600 [22:22:18<190:56:43, 480.03s/it]index: 0 loss: 0.30390137
index: 200 loss: 0.34377256
index: 400 loss: 0.25592902
index: 600 loss: 0.3375971
index: 800 loss: 0.33924836
epoch: 168 validateAccuracy: 0.4028 trainAccuracy: 0.9517 delta: -0.0001
Training:  11%|█         | 169/1600 [22:30:17<190:44:31, 479.85s/it]index: 0 loss: 0.258773
index: 200 loss: 0.28419712
index: 400 loss: 0.33642265
index: 600 loss: 0.28359002
index: 800 loss: 0.24420087
epoch: 169 validateAccuracy: 0.3995 trainAccuracy: 0.9448 delta: -0.0034
Training:  11%|█         | 170/1600 [22:38:15<190:21:40, 479.23s/it]index: 0 loss: 0.330828
index: 200 loss: 0.3397631
index: 400 loss: 0.24799696
index: 600 loss: 0.32395273
index: 800 loss: 0.35026532
epoch: 170 validateAccuracy: 0.4007 trainAccuracy: 0.935 delta: -0.0022
Training:  11%|█         | 171/1600 [22:46:15<190:16:22, 479.34s/it]index: 0 loss: 0.42341104
index: 200 loss: 0.32735708
index: 400 loss: 0.38021743
index: 600 loss: 0.27731752
index: 800 loss: 0.34384143
epoch: 171 validateAccuracy: 0.3963 trainAccuracy: 0.9227 delta: -0.0066
Training:  11%|█         | 172/1600 [22:54:13<190:00:42, 479.02s/it]index: 0 loss: 0.39928997
index: 200 loss: 0.47844398
index: 400 loss: 0.43099472
index: 600 loss: 0.4157753
index: 800 loss: 0.36546597
epoch: 172 validateAccuracy: 0.3963 trainAccuracy: 0.9073 delta: -0.0066
Training:  11%|█         | 173/1600 [23:02:11<189:46:29, 478.76s/it]index: 0 loss: 0.33287352
index: 200 loss: 0.42156166
index: 400 loss: 0.5680766
index: 600 loss: 0.36062035
index: 800 loss: 0.4534185
epoch: 173 validateAccuracy: 0.3915 trainAccuracy: 0.8908 delta: -0.0114
Training:  11%|█         | 174/1600 [23:10:09<189:31:25, 478.46s/it]index: 0 loss: 0.38886043
index: 200 loss: 0.48930335
index: 400 loss: 0.508613
index: 600 loss: 0.45646727
index: 800 loss: 0.704533
epoch: 174 validateAccuracy: 0.3888 trainAccuracy: 0.8728 delta: -0.0141
Training:  11%|█         | 175/1600 [23:18:03<188:53:14, 477.19s/it]index: 0 loss: 0.33624697
index: 200 loss: 0.3646806
index: 400 loss: 0.57653445
index: 600 loss: 0.61873156
index: 800 loss: 0.47209316
epoch: 175 validateAccuracy: 0.3837 trainAccuracy: 0.8595 delta: -0.0192
Training:  11%|█         | 176/1600 [23:25:57<188:20:20, 476.14s/it]index: 0 loss: 0.5753538
index: 200 loss: 0.6943099
index: 400 loss: 0.3846592
index: 600 loss: 0.72010875
index: 800 loss: 0.47709703
epoch: 176 validateAccuracy: 0.3751 trainAccuracy: 0.8472 delta: -0.0278
Training:  11%|█         | 177/1600 [23:33:53<188:13:27, 476.18s/it]index: 0 loss: 0.54651666
index: 200 loss: 0.76460445
index: 400 loss: 0.64604926
index: 600 loss: 0.72406393
index: 800 loss: 0.5298717
epoch: 177 validateAccuracy: 0.371 trainAccuracy: 0.8357 delta: -0.0319
Training:  11%|█         | 178/1600 [23:41:50<188:08:14, 476.30s/it]index: 0 loss: 0.66135895
index: 200 loss: 0.9322766
index: 400 loss: 0.57293767
index: 600 loss: 0.5689697
index: 800 loss: 0.55492574
epoch: 178 validateAccuracy: 0.3662 trainAccuracy: 0.8333 delta: -0.0367
Training:  11%|█         | 179/1600 [23:49:49<188:20:59, 477.17s/it]index: 0 loss: 0.71304476
index: 200 loss: 0.48518634
index: 400 loss: 0.47717178
index: 600 loss: 0.70404065
index: 800 loss: 0.65233797
epoch: 179 validateAccuracy: 0.3773 trainAccuracy: 0.8222 delta: -0.0256
Training:  11%|█▏        | 180/1600 [23:57:49<188:32:56, 478.01s/it]index: 0 loss: 0.7760903
index: 200 loss: 0.4480297
index: 400 loss: 0.5642333
index: 600 loss: 0.7405688
index: 800 loss: 0.66075855
epoch: 180 validateAccuracy: 0.3738 trainAccuracy: 0.8247 delta: -0.0291
Training:  11%|█▏        | 181/1600 [24:05:49<188:40:49, 478.68s/it]index: 0 loss: 0.48651564
index: 200 loss: 0.60136986
index: 400 loss: 0.7362757
index: 600 loss: 0.70890117
index: 800 loss: 0.6248683
epoch: 181 validateAccuracy: 0.3707 trainAccuracy: 0.8266 delta: -0.0322
Training:  11%|█▏        | 182/1600 [24:13:49<188:38:05, 478.90s/it]index: 0 loss: 0.6447064
index: 200 loss: 0.5218507
index: 400 loss: 0.5106835
index: 600 loss: 0.6435935
index: 800 loss: 0.6215725
epoch: 182 validateAccuracy: 0.3753 trainAccuracy: 0.8294 delta: -0.0276
Training:  11%|█▏        | 183/1600 [24:21:44<188:08:53, 478.01s/it]index: 0 loss: 0.6425462
index: 200 loss: 0.72953206
index: 400 loss: 0.70046335
index: 600 loss: 0.6124909
index: 800 loss: 0.5834643
epoch: 183 validateAccuracy: 0.3665 trainAccuracy: 0.8366 delta: -0.0364
Training:  12%|█▏        | 184/1600 [24:29:43<188:01:49, 478.04s/it]index: 0 loss: 0.6821033
index: 200 loss: 0.6985175
index: 400 loss: 0.67810273
index: 600 loss: 0.55346364
index: 800 loss: 0.5540417
epoch: 184 validateAccuracy: 0.3735 trainAccuracy: 0.8431 delta: -0.0294
Training:  12%|█▏        | 185/1600 [24:37:41<187:57:18, 478.19s/it]index: 0 loss: 0.66046584
index: 200 loss: 0.70870143
index: 400 loss: 0.5758368
index: 600 loss: 0.61523354
index: 800 loss: 0.78435284
epoch: 185 validateAccuracy: 0.3795 trainAccuracy: 0.8501 delta: -0.0234
Training:  12%|█▏        | 186/1600 [24:45:40<187:51:31, 478.28s/it]index: 0 loss: 0.4986217
index: 200 loss: 0.36740848
index: 400 loss: 0.62383354
index: 600 loss: 0.6760106
index: 800 loss: 0.5883974
epoch: 186 validateAccuracy: 0.3827 trainAccuracy: 0.8651 delta: -0.0202
Training:  12%|█▏        | 187/1600 [24:53:39<187:48:28, 478.49s/it]index: 0 loss: 0.5944958
index: 200 loss: 0.3468369
index: 400 loss: 0.37453416
index: 600 loss: 0.46707454
index: 800 loss: 0.441931
epoch: 187 validateAccuracy: 0.3876 trainAccuracy: 0.8771 delta: -0.0153
Training:  12%|█▏        | 188/1600 [25:01:38<187:43:44, 478.63s/it]index: 0 loss: 0.56798285
index: 200 loss: 0.49862403
index: 400 loss: 0.4708363
index: 600 loss: 0.4352541
index: 800 loss: 0.31632742
epoch: 188 validateAccuracy: 0.3897 trainAccuracy: 0.8915 delta: -0.0132
Training:  12%|█▏        | 189/1600 [25:09:37<187:39:33, 478.79s/it]index: 0 loss: 0.34773904
index: 200 loss: 0.35873967
index: 400 loss: 0.41705358
index: 600 loss: 0.39641267
index: 800 loss: 0.45563278
epoch: 189 validateAccuracy: 0.3988 trainAccuracy: 0.9038 delta: -0.0041
Training:  12%|█▏        | 190/1600 [25:17:36<187:37:47, 479.05s/it]index: 0 loss: 0.27583134
index: 200 loss: 0.28367376
index: 400 loss: 0.34617957
index: 600 loss: 0.36808985
index: 800 loss: 0.24469174
epoch: 190 validateAccuracy: 0.3905 trainAccuracy: 0.9166 delta: -0.0124
Training:  12%|█▏        | 191/1600 [25:25:35<187:29:14, 479.03s/it]index: 0 loss: 0.29691944
index: 200 loss: 0.31944063
index: 400 loss: 0.33481917
index: 600 loss: 0.45651755
index: 800 loss: 0.2635796
epoch: 191 validateAccuracy: 0.3969 trainAccuracy: 0.929 delta: -0.006
Training:  12%|█▏        | 192/1600 [25:33:36<187:31:34, 479.47s/it]index: 0 loss: 0.2612011
index: 200 loss: 0.28882158
index: 400 loss: 0.3221233
index: 600 loss: 0.26633173
index: 800 loss: 0.2688343
epoch: 192 validateAccuracy: 0.3985 trainAccuracy: 0.9404 delta: -0.0044
Training:  12%|█▏        | 193/1600 [25:41:36<187:29:12, 479.71s/it]index: 0 loss: 0.31714243
index: 200 loss: 0.28518036
index: 400 loss: 0.26910308
index: 600 loss: 0.3654489
index: 800 loss: 0.31345513
epoch: 193 validateAccuracy: 0.4012 trainAccuracy: 0.9488 delta: -0.0017
Training:  12%|█▏        | 194/1600 [25:49:36<187:23:07, 479.79s/it]index: 0 loss: 0.20957103
index: 200 loss: 0.24500705
index: 400 loss: 0.22931507
index: 600 loss: 0.30674255
index: 800 loss: 0.3078034
epoch: 194 validateAccuracy: 0.4004 trainAccuracy: 0.9561 delta: -0.0025
Training:  12%|█▏        | 195/1600 [25:57:35<187:10:07, 479.58s/it]index: 0 loss: 0.19897947
index: 200 loss: 0.22614431
index: 400 loss: 0.30045468
index: 600 loss: 0.2781207
index: 800 loss: 0.30813652
epoch: 195 validateAccuracy: 0.4015 trainAccuracy: 0.9591 delta: -0.0014
Training:  12%|█▏        | 196/1600 [26:05:35<187:00:36, 479.51s/it]index: 0 loss: 0.20561828
index: 200 loss: 0.3116954
index: 400 loss: 0.32653588
index: 600 loss: 0.23186518
index: 800 loss: 0.23442426
epoch: 196 validateAccuracy: 0.401 trainAccuracy: 0.9606 delta: -0.0019
Training:  12%|█▏        | 197/1600 [26:13:32<186:36:27, 478.82s/it]index: 0 loss: 0.22364889
index: 200 loss: 0.2473433
index: 400 loss: 0.2336835
index: 600 loss: 0.24653363
index: 800 loss: 0.1937988
epoch: 197 validateAccuracy: 0.4021 trainAccuracy: 0.96 delta: -0.0008
Training:  12%|█▏        | 198/1600 [26:21:30<186:23:48, 478.62s/it]index: 0 loss: 0.23774882
index: 200 loss: 0.25343764
index: 400 loss: 0.2798072
index: 600 loss: 0.3080076
index: 800 loss: 0.2836634
epoch: 198 validateAccuracy: 0.4024 trainAccuracy: 0.9565 delta: -0.0005
Training:  12%|█▏        | 199/1600 [26:29:30<186:25:06, 479.02s/it]index: 0 loss: 0.24122754
index: 200 loss: 0.25026774
index: 400 loss: 0.23335797
index: 600 loss: 0.3500239
index: 800 loss: 0.3295988
epoch: 199 validateAccuracy: 0.4049 trainAccuracy: 0.9514 delta: 0.002
Training:  12%|█▎        | 200/1600 [26:37:32<186:36:41, 479.86s/it]index: 0 loss: 0.26382834
index: 200 loss: 0.24944094
index: 400 loss: 0.25175717
index: 600 loss: 0.32256797
index: 800 loss: 0.31776184
epoch: 200 validateAccuracy: 0.3992 trainAccuracy: 0.9409 delta: -0.0057
Training:  13%|█▎        | 201/1600 [26:45:31<186:27:48, 479.82s/it]index: 0 loss: 0.2745133
index: 200 loss: 0.2792496
index: 400 loss: 0.33455363
index: 600 loss: 0.31603733
index: 800 loss: 0.30885178
epoch: 201 validateAccuracy: 0.3975 trainAccuracy: 0.93 delta: -0.0074
Training:  13%|█▎        | 202/1600 [26:53:32<186:23:51, 479.99s/it]index: 0 loss: 0.37529722
index: 200 loss: 0.3496749
index: 400 loss: 0.30906823
index: 600 loss: 0.4117103
index: 800 loss: 0.3132392
epoch: 202 validateAccuracy: 0.396 trainAccuracy: 0.9178 delta: -0.0089
Training:  13%|█▎        | 203/1600 [27:01:30<186:05:10, 479.54s/it]index: 0 loss: 0.3203592
index: 200 loss: 0.5722358
index: 400 loss: 0.36359748
index: 600 loss: 0.45691773
index: 800 loss: 0.35602272
epoch: 203 validateAccuracy: 0.391 trainAccuracy: 0.9005 delta: -0.0139
Training:  13%|█▎        | 204/1600 [27:09:29<185:48:12, 479.15s/it]index: 0 loss: 0.43087494
index: 200 loss: 0.4444308
index: 400 loss: 0.39282113
index: 600 loss: 0.38181615
index: 800 loss: 0.30047548
epoch: 204 validateAccuracy: 0.3827 trainAccuracy: 0.8878 delta: -0.0222
Training:  13%|█▎        | 205/1600 [27:17:28<185:44:42, 479.34s/it]index: 0 loss: 0.36866617
index: 200 loss: 0.5246352
index: 400 loss: 0.4945014
index: 600 loss: 0.34690502
index: 800 loss: 0.5473699
epoch: 205 validateAccuracy: 0.3805 trainAccuracy: 0.8708 delta: -0.0244
Training:  13%|█▎        | 206/1600 [27:25:29<185:44:04, 479.66s/it]index: 0 loss: 0.35271913
index: 200 loss: 0.60113955
index: 400 loss: 0.6852543
index: 600 loss: 0.66423506
index: 800 loss: 0.4975014
epoch: 206 validateAccuracy: 0.3819 trainAccuracy: 0.8609 delta: -0.023
Training:  13%|█▎        | 207/1600 [27:33:30<185:44:33, 480.02s/it]index: 0 loss: 0.41292298
index: 200 loss: 0.42160663
index: 400 loss: 0.61736214
index: 600 loss: 0.5947134
index: 800 loss: 0.44014567
epoch: 207 validateAccuracy: 0.3722 trainAccuracy: 0.8512 delta: -0.0327
Training:  13%|█▎        | 208/1600 [27:41:28<185:25:15, 479.54s/it]index: 0 loss: 0.47787893
index: 200 loss: 0.4926295
index: 400 loss: 0.5391224
index: 600 loss: 0.6144893
index: 800 loss: 0.5663857
epoch: 208 validateAccuracy: 0.3701 trainAccuracy: 0.8452 delta: -0.0348
Training:  13%|█▎        | 209/1600 [27:49:26<185:07:48, 479.13s/it]index: 0 loss: 0.5786189
index: 200 loss: 0.53377205
index: 400 loss: 0.5875847
index: 600 loss: 0.638048
index: 800 loss: 0.5728812
epoch: 209 validateAccuracy: 0.3726 trainAccuracy: 0.8399 delta: -0.0323
Training:  13%|█▎        | 210/1600 [27:57:27<185:10:26, 479.59s/it]index: 0 loss: 0.5752213
index: 200 loss: 0.58860505
index: 400 loss: 0.5928812
index: 600 loss: 0.6437082
index: 800 loss: 0.75058717
epoch: 210 validateAccuracy: 0.3788 trainAccuracy: 0.8412 delta: -0.0261
Training:  13%|█▎        | 211/1600 [28:05:27<185:08:28, 479.85s/it]index: 0 loss: 0.6025511
index: 200 loss: 0.71043694
index: 400 loss: 0.5633856
index: 600 loss: 0.59143907
index: 800 loss: 0.7030923
epoch: 211 validateAccuracy: 0.3746 trainAccuracy: 0.8372 delta: -0.0303
Training:  13%|█▎        | 212/1600 [28:13:25<184:47:38, 479.29s/it]index: 0 loss: 0.6205199
index: 200 loss: 0.6748866
index: 400 loss: 0.48457643
index: 600 loss: 0.46806523
index: 800 loss: 0.492243
epoch: 212 validateAccuracy: 0.3828 trainAccuracy: 0.8432 delta: -0.0221
Training:  13%|█▎        | 213/1600 [28:21:24<184:32:40, 478.99s/it]index: 0 loss: 0.47259042
index: 200 loss: 0.4896531
index: 400 loss: 0.378321
index: 600 loss: 0.5385331
index: 800 loss: 0.43636018
epoch: 213 validateAccuracy: 0.3759 trainAccuracy: 0.8496 delta: -0.029
Training:  13%|█▎        | 214/1600 [28:29:22<184:20:16, 478.80s/it]index: 0 loss: 0.6495592
index: 200 loss: 0.54734635
index: 400 loss: 0.45096844
index: 600 loss: 0.661085
index: 800 loss: 0.49971992
epoch: 214 validateAccuracy: 0.3966 trainAccuracy: 0.8555 delta: -0.0083
Training:  13%|█▎        | 215/1600 [28:37:21<184:13:21, 478.85s/it]index: 0 loss: 0.50488156
index: 200 loss: 0.42265671
index: 400 loss: 0.5719791
index: 600 loss: 0.4725447
index: 800 loss: 0.5738576
epoch: 215 validateAccuracy: 0.391 trainAccuracy: 0.8629 delta: -0.0139
Training:  14%|█▎        | 216/1600 [28:45:19<184:01:35, 478.68s/it]index: 0 loss: 0.4768207
index: 200 loss: 0.28663474
index: 400 loss: 0.54018134
index: 600 loss: 0.47834185
index: 800 loss: 0.5001987
epoch: 216 validateAccuracy: 0.3858 trainAccuracy: 0.8741 delta: -0.0191
Training:  14%|█▎        | 217/1600 [28:53:18<183:55:10, 478.75s/it]index: 0 loss: 0.4269503
index: 200 loss: 0.436837
index: 400 loss: 0.566224
index: 600 loss: 0.40741718
index: 800 loss: 0.44471034
epoch: 217 validateAccuracy: 0.3728 trainAccuracy: 0.887 delta: -0.0321
Training:  14%|█▎        | 218/1600 [29:01:17<183:47:32, 478.76s/it]index: 0 loss: 0.40946215
index: 200 loss: 0.38955167
index: 400 loss: 0.46350047
index: 600 loss: 0.3619444
index: 800 loss: 0.4216856
epoch: 218 validateAccuracy: 0.3871 trainAccuracy: 0.9004 delta: -0.0178
Training:  14%|█▎        | 219/1600 [29:09:16<183:45:03, 479.00s/it]index: 0 loss: 0.2868311
index: 200 loss: 0.4198443
index: 400 loss: 0.27427134
index: 600 loss: 0.3678449
index: 800 loss: 0.3870078
epoch: 219 validateAccuracy: 0.3963 trainAccuracy: 0.9097 delta: -0.0086
Training:  14%|█▍        | 220/1600 [29:17:17<183:47:10, 479.44s/it]index: 0 loss: 0.36137298
index: 200 loss: 0.39328626
index: 400 loss: 0.35825866
index: 600 loss: 0.28961745
index: 800 loss: 0.25442272
epoch: 220 validateAccuracy: 0.3994 trainAccuracy: 0.9223 delta: -0.0055
Training:  14%|█▍        | 221/1600 [29:25:15<183:32:18, 479.14s/it]index: 0 loss: 0.35044822
index: 200 loss: 0.3545744
index: 400 loss: 0.3450218
index: 600 loss: 0.24616387
index: 800 loss: 0.35457546
epoch: 221 validateAccuracy: 0.398 trainAccuracy: 0.9348 delta: -0.0069
Training:  14%|█▍        | 222/1600 [29:33:13<183:16:28, 478.80s/it]index: 0 loss: 0.32178816
index: 200 loss: 0.26545468
index: 400 loss: 0.23179051
index: 600 loss: 0.24466291
index: 800 loss: 0.3267515
epoch: 222 validateAccuracy: 0.4005 trainAccuracy: 0.9453 delta: -0.0044
Training:  14%|█▍        | 223/1600 [29:41:11<182:59:09, 478.39s/it]index: 0 loss: 0.2472261
index: 200 loss: 0.19646293
index: 400 loss: 0.23206452
index: 600 loss: 0.28755313
index: 800 loss: 0.29589826
epoch: 223 validateAccuracy: 0.4035 trainAccuracy: 0.9518 delta: -0.0014
Training:  14%|█▍        | 224/1600 [29:49:09<182:50:37, 478.37s/it]index: 0 loss: 0.2804119
index: 200 loss: 0.20145868
index: 400 loss: 0.20590407
index: 600 loss: 0.31687903
index: 800 loss: 0.2721413
epoch: 224 validateAccuracy: 0.4013 trainAccuracy: 0.9597 delta: -0.0036
Training:  14%|█▍        | 225/1600 [29:57:08<182:47:08, 478.57s/it]index: 0 loss: 0.27972275
index: 200 loss: 0.19945039
index: 400 loss: 0.16462256
index: 600 loss: 0.2541581
index: 800 loss: 0.3270886
epoch: 225 validateAccuracy: 0.4022 trainAccuracy: 0.9634 delta: -0.0027
Training:  14%|█▍        | 226/1600 [30:05:08<182:44:31, 478.80s/it]index: 0 loss: 0.21914059
index: 200 loss: 0.24942495
index: 400 loss: 0.19357438
index: 600 loss: 0.3398881
index: 800 loss: 0.20708644
epoch: 226 validateAccuracy: 0.4022 trainAccuracy: 0.9647 delta: -0.0027
Training:  14%|█▍        | 227/1600 [30:13:07<182:38:24, 478.88s/it]index: 0 loss: 0.2827288
index: 200 loss: 0.18636325
index: 400 loss: 0.27443537
index: 600 loss: 0.20258091
index: 800 loss: 0.28530806
epoch: 227 validateAccuracy: 0.4027 trainAccuracy: 0.963 delta: -0.0022
Training:  14%|█▍        | 228/1600 [30:21:06<182:31:24, 478.92s/it]index: 0 loss: 0.23095049
index: 200 loss: 0.23347643
index: 400 loss: 0.21192932
index: 600 loss: 0.23811415
index: 800 loss: 0.27156803
epoch: 228 validateAccuracy: 0.4051 trainAccuracy: 0.9604 delta: 0.0002
Training:  14%|█▍        | 229/1600 [30:29:07<182:39:27, 479.63s/it]index: 0 loss: 0.20723648
index: 200 loss: 0.27946344
index: 400 loss: 0.24384695
index: 600 loss: 0.2430254
index: 800 loss: 0.25862336
epoch: 229 validateAccuracy: 0.4015 trainAccuracy: 0.9545 delta: -0.0036
Training:  14%|█▍        | 230/1600 [30:37:03<182:07:00, 478.56s/it]index: 0 loss: 0.18343028
index: 200 loss: 0.28621036
index: 400 loss: 0.2655785
index: 600 loss: 0.30183178
index: 800 loss: 0.3340614
epoch: 230 validateAccuracy: 0.4017 trainAccuracy: 0.9463 delta: -0.0034
Training:  14%|█▍        | 231/1600 [30:45:00<181:50:15, 478.17s/it]index: 0 loss: 0.22839686
index: 200 loss: 0.2637358
index: 400 loss: 0.30903572
index: 600 loss: 0.36303756
index: 800 loss: 0.30232102
epoch: 231 validateAccuracy: 0.4017 trainAccuracy: 0.9356 delta: -0.0034
Training:  14%|█▍        | 232/1600 [30:52:58<181:40:38, 478.10s/it]index: 0 loss: 0.28489938
index: 200 loss: 0.34796914
index: 400 loss: 0.36233735
index: 600 loss: 0.35660967
index: 800 loss: 0.36842027
epoch: 232 validateAccuracy: 0.3936 trainAccuracy: 0.9215 delta: -0.0115
Training:  15%|█▍        | 233/1600 [31:00:56<181:31:41, 478.06s/it]index: 0 loss: 0.28030086
index: 200 loss: 0.38326165
index: 400 loss: 0.38442641
index: 600 loss: 0.39632264
index: 800 loss: 0.40298298
epoch: 233 validateAccuracy: 0.3942 trainAccuracy: 0.9097 delta: -0.0109
Training:  15%|█▍        | 234/1600 [31:08:55<181:27:03, 478.20s/it]index: 0 loss: 0.39818537
index: 200 loss: 0.43904078
index: 400 loss: 0.37996593
index: 600 loss: 0.36709198
index: 800 loss: 0.37151107
epoch: 234 validateAccuracy: 0.3923 trainAccuracy: 0.8933 delta: -0.0128
Training:  15%|█▍        | 235/1600 [31:16:54<181:25:22, 478.48s/it]index: 0 loss: 0.310338
index: 200 loss: 0.46327808
index: 400 loss: 0.44637772
index: 600 loss: 0.3697425
index: 800 loss: 0.50239307
epoch: 235 validateAccuracy: 0.3896 trainAccuracy: 0.8848 delta: -0.0155
Training:  15%|█▍        | 236/1600 [31:24:51<181:05:39, 477.96s/it]index: 0 loss: 0.38296708
index: 200 loss: 0.42714974
index: 400 loss: 0.47839636
index: 600 loss: 0.3949942
index: 800 loss: 0.5042723
epoch: 236 validateAccuracy: 0.3791 trainAccuracy: 0.8725 delta: -0.026
Training:  15%|█▍        | 237/1600 [31:33:00<182:13:18, 481.29s/it]index: 0 loss: 0.36094734
index: 200 loss: 0.6426759
index: 400 loss: 0.57511145
index: 600 loss: 0.77054816
index: 800 loss: 0.48863843
epoch: 237 validateAccuracy: 0.3802 trainAccuracy: 0.8635 delta: -0.0249
Training:  15%|█▍        | 238/1600 [31:40:57<181:40:46, 480.21s/it]index: 0 loss: 0.4910437
index: 200 loss: 0.7578884
index: 400 loss: 0.6083592
index: 600 loss: 0.4758808
index: 800 loss: 0.66942173
epoch: 238 validateAccuracy: 0.3769 trainAccuracy: 0.8533 delta: -0.0282
Training:  15%|█▍        | 239/1600 [31:48:55<181:15:36, 479.45s/it]index: 0 loss: 0.49536812
index: 200 loss: 0.7204299
index: 400 loss: 0.47644562
index: 600 loss: 0.6313282
index: 800 loss: 0.5862635
epoch: 239 validateAccuracy: 0.3828 trainAccuracy: 0.8508 delta: -0.0223
Training:  15%|█▌        | 240/1600 [31:56:53<181:01:07, 479.17s/it]index: 0 loss: 0.48196283
index: 200 loss: 0.7001647
index: 400 loss: 0.7292806
index: 600 loss: 0.51965714
index: 800 loss: 0.5802717
epoch: 240 validateAccuracy: 0.3731 trainAccuracy: 0.8511 delta: -0.032
Training:  15%|█▌        | 241/1600 [32:04:49<180:27:53, 478.05s/it]index: 0 loss: 0.51438355
index: 200 loss: 0.53180695
index: 400 loss: 0.68440306
index: 600 loss: 0.43342716
index: 800 loss: 0.5146405
epoch: 241 validateAccuracy: 0.3697 trainAccuracy: 0.8488 delta: -0.0354
Training:  15%|█▌        | 242/1600 [32:12:49<180:31:42, 478.57s/it]index: 0 loss: 0.37984392
index: 200 loss: 0.45990416
index: 400 loss: 0.5479771
index: 600 loss: 0.5723931
index: 800 loss: 0.50807405
epoch: 242 validateAccuracy: 0.3739 trainAccuracy: 0.8538 delta: -0.0312
Training:  15%|█▌        | 243/1600 [32:20:49<180:34:48, 479.06s/it]index: 0 loss: 0.45017573
index: 200 loss: 0.6147292
index: 400 loss: 0.60199136
index: 600 loss: 0.62747365
index: 800 loss: 0.48991802
epoch: 243 validateAccuracy: 0.3859 trainAccuracy: 0.8574 delta: -0.0192
Training:  15%|█▌        | 244/1600 [32:28:45<180:08:09, 478.24s/it]index: 0 loss: 0.49775684
index: 200 loss: 0.54329056
index: 400 loss: 0.40510106
index: 600 loss: 0.602974
index: 800 loss: 0.6039701
epoch: 244 validateAccuracy: 0.3789 trainAccuracy: 0.8628 delta: -0.0262
Training:  15%|█▌        | 245/1600 [32:36:44<180:04:54, 478.45s/it]index: 0 loss: 0.34040496
index: 200 loss: 0.5487657
index: 400 loss: 0.47545868
index: 600 loss: 0.50093615
index: 800 loss: 0.62462723
epoch: 245 validateAccuracy: 0.3791 trainAccuracy: 0.8752 delta: -0.026
Training:  15%|█▌        | 246/1600 [32:44:43<179:59:42, 478.57s/it]index: 0 loss: 0.51346326
index: 200 loss: 0.5108582
index: 400 loss: 0.51709276
index: 600 loss: 0.38438505
index: 800 loss: 0.40172234
epoch: 246 validateAccuracy: 0.376 trainAccuracy: 0.8852 delta: -0.0291
Training:  15%|█▌        | 247/1600 [32:52:41<179:45:32, 478.29s/it]index: 0 loss: 0.23470846
index: 200 loss: 0.3290706
index: 400 loss: 0.47629985
index: 600 loss: 0.30088738
index: 800 loss: 0.34844398
epoch: 247 validateAccuracy: 0.3915 trainAccuracy: 0.895 delta: -0.0136
Training:  16%|█▌        | 248/1600 [33:00:41<179:54:41, 479.05s/it]index: 0 loss: 0.34976795
index: 200 loss: 0.27944872
index: 400 loss: 0.4526386
index: 600 loss: 0.333175
index: 800 loss: 0.2432577
epoch: 248 validateAccuracy: 0.3952 trainAccuracy: 0.9089 delta: -0.0099
Training:  16%|█▌        | 249/1600 [33:08:41<179:51:06, 479.25s/it]index: 0 loss: 0.45041078
index: 200 loss: 0.4336869
index: 400 loss: 0.42496887
index: 600 loss: 0.32802168
index: 800 loss: 0.2285484
epoch: 249 validateAccuracy: 0.3909 trainAccuracy: 0.9189 delta: -0.0142
Training:  16%|█▌        | 250/1600 [33:16:39<179:36:14, 478.94s/it]index: 0 loss: 0.38143525
index: 200 loss: 0.27422872
index: 400 loss: 0.28146952
index: 600 loss: 0.3460741
index: 800 loss: 0.35307592
epoch: 250 validateAccuracy: 0.3927 trainAccuracy: 0.929 delta: -0.0124
Training:  16%|█▌        | 251/1600 [33:24:38<179:25:52, 478.84s/it]index: 0 loss: 0.35304174
index: 200 loss: 0.3005949
index: 400 loss: 0.37254697
index: 600 loss: 0.29892638
index: 800 loss: 0.32473674
epoch: 251 validateAccuracy: 0.3991 trainAccuracy: 0.9391 delta: -0.006
Training:  16%|█▌        | 252/1600 [33:32:35<179:04:23, 478.24s/it]index: 0 loss: 0.22600676
index: 200 loss: 0.20210719
index: 400 loss: 0.2902032
index: 600 loss: 0.23539737
index: 800 loss: 0.22805172
epoch: 252 validateAccuracy: 0.3975 trainAccuracy: 0.9501 delta: -0.0076
Training:  16%|█▌        | 253/1600 [33:40:35<179:10:40, 478.87s/it]index: 0 loss: 0.26493928
index: 200 loss: 0.21624605
index: 400 loss: 0.26046705
index: 600 loss: 0.2960395
index: 800 loss: 0.26483148
epoch: 253 validateAccuracy: 0.3957 trainAccuracy: 0.9562 delta: -0.0094
Training:  16%|█▌        | 254/1600 [33:48:35<179:06:13, 479.03s/it]index: 0 loss: 0.19414677
index: 200 loss: 0.16156773
index: 400 loss: 0.20242602
index: 600 loss: 0.17218213
index: 800 loss: 0.25902906
epoch: 254 validateAccuracy: 0.3993 trainAccuracy: 0.9627 delta: -0.0058
Training:  16%|█▌        | 255/1600 [33:56:33<178:57:20, 478.99s/it]index: 0 loss: 0.2793032
index: 200 loss: 0.22109559
index: 400 loss: 0.2334571
index: 600 loss: 0.22522517
index: 800 loss: 0.26140767
epoch: 255 validateAccuracy: 0.3986 trainAccuracy: 0.9645 delta: -0.0065
Training:  16%|█▌        | 256/1600 [34:04:32<178:49:08, 478.98s/it]index: 0 loss: 0.1891061
index: 200 loss: 0.23767227
index: 400 loss: 0.18499592
index: 600 loss: 0.18450662
index: 800 loss: 0.29716283
epoch: 256 validateAccuracy: 0.3978 trainAccuracy: 0.9655 delta: -0.0073
Training:  16%|█▌        | 257/1600 [34:12:32<178:47:19, 479.26s/it]index: 0 loss: 0.20145984
index: 200 loss: 0.20540847
index: 400 loss: 0.23631997
index: 600 loss: 0.14551215
index: 800 loss: 0.15684418
epoch: 257 validateAccuracy: 0.3996 trainAccuracy: 0.9663 delta: -0.0055
Training:  16%|█▌        | 258/1600 [34:20:33<178:51:03, 479.78s/it]index: 0 loss: 0.23610456
index: 200 loss: 0.19255318
index: 400 loss: 0.26938716
index: 600 loss: 0.23250978
index: 800 loss: 0.17524447
epoch: 258 validateAccuracy: 0.3987 trainAccuracy: 0.9648 delta: -0.0064
Training:  16%|█▌        | 259/1600 [34:28:33<178:42:54, 479.77s/it]index: 0 loss: 0.22467911
index: 200 loss: 0.17874932
index: 400 loss: 0.19532748
index: 600 loss: 0.26761422
index: 800 loss: 0.253581
epoch: 259 validateAccuracy: 0.3985 trainAccuracy: 0.9585 delta: -0.0066
Training:  16%|█▋        | 260/1600 [34:36:32<178:26:19, 479.39s/it]index: 0 loss: 0.2056535
index: 200 loss: 0.26308173
index: 400 loss: 0.24336459
index: 600 loss: 0.26521552
index: 800 loss: 0.36104736
epoch: 260 validateAccuracy: 0.3942 trainAccuracy: 0.9512 delta: -0.0109
Training:  16%|█▋        | 261/1600 [34:44:31<178:15:40, 479.27s/it]index: 0 loss: 0.20813678
index: 200 loss: 0.27802125
index: 400 loss: 0.19178616
index: 600 loss: 0.33383587
index: 800 loss: 0.2465042
epoch: 261 validateAccuracy: 0.3942 trainAccuracy: 0.941 delta: -0.0109
Training:  16%|█▋        | 262/1600 [34:52:29<177:59:14, 478.89s/it]index: 0 loss: 0.26311076
index: 200 loss: 0.4748343
index: 400 loss: 0.3049825
index: 600 loss: 0.35586512
index: 800 loss: 0.35149598
epoch: 262 validateAccuracy: 0.3965 trainAccuracy: 0.9282 delta: -0.0086
Training:  16%|█▋        | 263/1600 [35:00:28<177:57:46, 479.18s/it]index: 0 loss: 0.43900916
index: 200 loss: 0.37488073
index: 400 loss: 0.40817848
index: 600 loss: 0.34842
index: 800 loss: 0.34024805
epoch: 263 validateAccuracy: 0.3823 trainAccuracy: 0.915 delta: -0.0228
Training:  16%|█▋        | 264/1600 [35:08:26<177:38:33, 478.68s/it]index: 0 loss: 0.3697715
index: 200 loss: 0.2619776
index: 400 loss: 0.32269245
index: 600 loss: 0.42807782
index: 800 loss: 0.41178384
epoch: 264 validateAccuracy: 0.388 trainAccuracy: 0.9025 delta: -0.0171
Training:  17%|█▋        | 265/1600 [35:16:24<177:26:27, 478.49s/it]index: 0 loss: 0.32918847
index: 200 loss: 0.46926737
index: 400 loss: 0.47234613
index: 600 loss: 0.40416226
index: 800 loss: 0.33025193
epoch: 265 validateAccuracy: 0.3913 trainAccuracy: 0.8915 delta: -0.0138
Training:  17%|█▋        | 266/1600 [35:24:24<177:25:38, 478.81s/it]index: 0 loss: 0.3599505
index: 200 loss: 0.42778814
index: 400 loss: 0.37896377
index: 600 loss: 0.5379232
index: 800 loss: 0.4868284
epoch: 266 validateAccuracy: 0.3809 trainAccuracy: 0.8805 delta: -0.0242
Training:  17%|█▋        | 267/1600 [35:32:21<177:06:39, 478.32s/it]index: 0 loss: 0.44754305
index: 200 loss: 0.60169786
index: 400 loss: 0.5326937
index: 600 loss: 0.46820423
index: 800 loss: 0.45963874
epoch: 267 validateAccuracy: 0.3728 trainAccuracy: 0.8722 delta: -0.0323
Training:  17%|█▋        | 268/1600 [35:40:20<177:03:39, 478.54s/it]index: 0 loss: 0.55626816
index: 200 loss: 0.48422423
index: 400 loss: 0.5426305
index: 600 loss: 0.37923816
index: 800 loss: 0.638288
epoch: 268 validateAccuracy: 0.3789 trainAccuracy: 0.8644 delta: -0.0262
Training:  17%|█▋        | 269/1600 [35:48:17<176:47:30, 478.17s/it]index: 0 loss: 0.50824994
index: 200 loss: 0.748813
index: 400 loss: 0.5921339
index: 600 loss: 0.8009142
index: 800 loss: 0.5545679
epoch: 269 validateAccuracy: 0.3771 trainAccuracy: 0.8576 delta: -0.028
Training:  17%|█▋        | 270/1600 [35:56:14<176:31:13, 477.80s/it]index: 0 loss: 0.6152361
index: 200 loss: 0.6782334
index: 400 loss: 0.58299863
index: 600 loss: 0.65163696
index: 800 loss: 0.46425205
epoch: 270 validateAccuracy: 0.3809 trainAccuracy: 0.8577 delta: -0.0242
Training:  17%|█▋        | 271/1600 [36:04:13<176:27:36, 478.00s/it]index: 0 loss: 0.5039024
index: 200 loss: 0.45668763
index: 400 loss: 0.46964538
index: 600 loss: 0.49760976
index: 800 loss: 0.5067427
epoch: 271 validateAccuracy: 0.3725 trainAccuracy: 0.8584 delta: -0.0326
Training:  17%|█▋        | 272/1600 [36:12:11<176:20:24, 478.03s/it]index: 0 loss: 0.47574392
index: 200 loss: 0.48405394
index: 400 loss: 0.5365178
index: 600 loss: 0.59563863
index: 800 loss: 0.4548913
epoch: 272 validateAccuracy: 0.3757 trainAccuracy: 0.8639 delta: -0.0294
Training:  17%|█▋        | 273/1600 [36:20:06<175:57:07, 477.34s/it]index: 0 loss: 0.46498477
index: 200 loss: 0.35361785
index: 400 loss: 0.51431197
index: 600 loss: 0.43028113
index: 800 loss: 0.40433222
epoch: 273 validateAccuracy: 0.3757 trainAccuracy: 0.8665 delta: -0.0294
Training:  17%|█▋        | 274/1600 [36:28:04<175:52:33, 477.49s/it]index: 0 loss: 0.6925264
index: 200 loss: 0.45892233
index: 400 loss: 0.5024195
index: 600 loss: 0.48280773
index: 800 loss: 0.37387803
epoch: 274 validateAccuracy: 0.3681 trainAccuracy: 0.8742 delta: -0.037
Training:  17%|█▋        | 275/1600 [36:36:02<175:44:40, 477.49s/it]index: 0 loss: 0.5056218
index: 200 loss: 0.4076852
index: 400 loss: 0.51490307
index: 600 loss: 0.50799376
index: 800 loss: 0.420934
epoch: 275 validateAccuracy: 0.393 trainAccuracy: 0.8831 delta: -0.0121
Training:  17%|█▋        | 276/1600 [36:44:01<175:49:58, 478.10s/it]index: 0 loss: 0.34506676
index: 200 loss: 0.35907507
index: 400 loss: 0.34828514
index: 600 loss: 0.39496714
index: 800 loss: 0.3818274
epoch: 276 validateAccuracy: 0.3927 trainAccuracy: 0.8929 delta: -0.0124
Training:  17%|█▋        | 277/1600 [36:52:00<175:45:09, 478.24s/it]index: 0 loss: 0.44647297
index: 200 loss: 0.3213526
index: 400 loss: 0.43147466
index: 600 loss: 0.40334755
index: 800 loss: 0.38991624
epoch: 277 validateAccuracy: 0.3877 trainAccuracy: 0.9015 delta: -0.0174
Training:  17%|█▋        | 278/1600 [36:59:55<175:19:38, 477.44s/it]index: 0 loss: 0.295191
index: 200 loss: 0.28735745
index: 400 loss: 0.3766322
index: 600 loss: 0.3673604
index: 800 loss: 0.50913715
epoch: 278 validateAccuracy: 0.3921 trainAccuracy: 0.9148 delta: -0.013
Training:  17%|█▋        | 278/1600 [37:07:52<176:34:27, 480.84s/it]
Loading data...
[38;5;2m[i 0518 01:35:43.261410 96 dataset.py:631] Found 5001 classes and 60012 images.[m
[38;5;2m[i 0518 01:35:44.541050 96 dataset.py:631] Found 5001 classes and 10002 images.[m
[38;5;2m[i 0518 01:35:46.539775 96 dataset.py:631] Found 5001 classes and 10002 images.[m
==========================================================================================
testAccuracy 0.3939 validateAccuracy 0.4051 bestEpoch 228 stopEpoch 278
==========================================================================================
